{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Programming With OpenACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the lab is intended for C/C++ programmers. The Fortran version of this lab is available [here](../Fortran/README.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will receive a warning five minutes before the lab instance shuts down. Remember to save your work! If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later.\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's execute the cell below to display information about the GPUs running on the server.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  5 14:23:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GH200 120GB             On  |   00000009:01:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             94W /  900W |       2MiB /  97871MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GH200 120GB             On  |   00000019:01:00.0 Off |                    0 |\n",
      "| N/A   46C    P0            111W /  900W |       1MiB /  97871MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our goal for this lab is to learn how to run our code on a GPU (Graphical Processing Unit).\n",
    "  \n",
    "  \n",
    "  \n",
    "![development_cycle.png](../images/development_cycle.png)\n",
    "\n",
    "This is the OpenACC 3-Step development cycle.\n",
    "\n",
    "**Analyze** your code, and predict where potential parallelism can be uncovered. Use profiler to help understand what is happening in the code, and where parallelism may exist.\n",
    "\n",
    "**Parallelize** your code, starting with the most time consuming parts. Focus on maintaining correct results from your program.\n",
    "\n",
    "**Optimize** your code, focusing on maximizing performance. Performance may not increase all-at-once during early parallelization.\n",
    "\n",
    "We are currently tackling the **parallelize** step. We have parallelized our code for a multicore CPU, and now we will learn what we need to do to get it running on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the Code (Multicore)\n",
    "\n",
    "We have already completed a basic multicore implementation of our lab code. Run the following script IF you would prefer to use the *parallel directive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/multicore/laplace2d.c ./laplace2d.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If you would prefer to use the kernels directive, run the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/multicore/kernels/laplace2d.c ./laplace2d.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Then you may run the multicore code by running the following script. An executable called **laplace_multicore** will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "main:\n",
      "     60, Generating copyout(Anew[:m*n]) [if not already present]\n",
      "         Generating copyin(A[:m*n]) [if not already present]\n",
      "laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,m,n)\n",
      "         Generating NVIDIA GPU code\n",
      "         38, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "             Generating reduction(max:error)\n",
      "         40,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "swap:\n",
      "     51, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,n,m)\n",
      "         Generating NVIDIA GPU code\n",
      "         53, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         55,   /* blockIdx.x threadIdx.x collapsed */\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 1.051388 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -acc -gpu=ccnative -Minfo=accel -o laplace_multicore jacobi.c laplace2d.c && ./laplace_multicore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Analyze the Code\n",
    "\n",
    "If you would like a refresher on the code files that we are working on, you may view both of them using the two links below.\n",
    "\n",
    "[jacobi.c](jacobi.c)  \n",
    "[laplace2d.c](laplace2d.c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code\n",
    "\n",
    "If you would like to profile your code with Nsight Systems, please follow the instructions in **[Lab2](../../../module2/English/C/README.ipynb#profilecode)**, and add NVTX to your code to manually instrument the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Introduction to GPUs (Graphical Processing Units)\n",
    "\n",
    "GPUs were originally used to render computer graphics for video games. While they continue to dominate the video game hardware market, GPUs have also been adopted as **high-throughput parallel hardware**. They excel at doing many things simultaneously.\n",
    "\n",
    "![cpu_with_gpu.png](../images/cpu_with_gpu.png)\n",
    "\n",
    "Similar to a multicore CPU, a GPU has multiple computational cores - these cores are less powerful when compared to a CPU core, so individually perform relatively poorly especially on a serial code. However, a typical GPU has 1000s of these cores, and when they are able to work together on a problem in parallel, we can see orders of magnitude speedup over CPUs for a range of algorithms. The programming model we adopt in accelerated applications is to offload the computationally expensive, parallelisable parts of the code onto the GPU, and the sequential parts of the code will continue to run on the CPU.\n",
    "\n",
    "GPUs are what is known as a SIMD architecture (SIMD stands for: single instruction, multiple data). This means that GPUs excel at taking a single computer instruction (such as a mathematical instruction, or a memory read/write) and applying that instruction to a large amount of data. Ultimately, this means that a GPU can execute thousands of operations at the same time. This functionality is in some ways similar to multicore CPU architecture, but of course with a GPU, we have many more cores at our disposal, and instructions are simultaneously issued to groups of threads running on those cores rather than per thread. Also worth noting is that the GPU memory operates typically at a much higher bandwidth than CPU memory. Many applications are bandwidth-bound i.e. limited by the speed that data can be accessed from memory, so GPUs are well-suited to help accelerate those applications as well.\n",
    "\n",
    "![cpu_and_gpu_diagram.png](../images/cpu_and_gpu_diagram.png)\n",
    "\n",
    "This diagram represents a machine that contains a CPU and a GPU. We can see that the CPU and GPU are two complete seperate devices, connected via an I/O Bus. This bus is traditionally a PCI-e bus, however, NVLink is a newer, faster alternative. These two devices also have seperate memory. This means that during the execution of our program, some amount of data will be transferred between the CPU and the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Management With OpenACC\n",
    "\n",
    "When programming for a GPU or similar architecture, where the device memory is distinct from the host CPU memory, we need to consider data management between the host and the device. Even with NVLink there is still a time cost to moving data between the CPU and the GPU, and this can become a limiter on our application performance, so we need to consider ways of mitigating this, some of which will be touched on in this and the next lab. With OpenACC the programmer can explicitly define data management by using the OpenACC data directive and data clauses, or, they can allow the compiler to handle the data management for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenACC Data Clauses\n",
    "\n",
    "Data clauses allow the programmer to specify data transfers between the host and device (or in our case, the CPU and the GPU). Let's look at an example where we do not use a data clause.\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "#pragma acc parallel loop\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 0;\n",
    "}\n",
    "```\n",
    "\n",
    "We have allocated an array `A` outside of our parallel region. This means that `A` is allocated in the CPU memory. However, we access `A` inside of our loop, and that loop is contained within a `parallel` region. Within that parallel region, `A[i]` is attempting to access a memory location within the GPU memory. We didn't explicitly allocate `A` on the GPU, so one of two things will happen.\n",
    "\n",
    "1. The compiler will understand what we are trying to do, and automatically copy **A** from the CPU to the GPU.\n",
    "2. The program will check for an array **A** in GPU memory, it won't find it, and it will throw an error.\n",
    "\n",
    "Instead of hoping that we have a compiler that can figure this out, we could instead use a **data clause**.\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "#pragma acc parallel loop copy(A[0:N])\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 0;\n",
    "}\n",
    "```\n",
    "\n",
    "We will learn the `copy` data clause first, because it is the easiest to use. We’ll look at the syntax in more detail shortly, but for now, understand that with the inclusion of the `copy` data clause, our program will now copy the content of `A` from the CPU memory, into GPU memory. Then, during the execution of the loop, it will properly access `A` from the GPU memory. After the parallel region is finished, our program will copy `A` from the GPU memory back to the CPU memory. Let's look at one more direct example.\n",
    "\n",
    "```cpp\n",
    "int *A = (int*) malloc(N * sizeof(int));\n",
    "\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 0;\n",
    "}\n",
    "\n",
    "#pragma acc parallel loop copy(A[0:N])\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 1;\n",
    "}\n",
    "```\n",
    "\n",
    "Now we have two loops; the first loop will execute on the CPU (since it does not have an OpenACC *parallel directive*), and the second loop will execute on the GPU. Array `A` will be allocated on the CPU, and then the first loop will execute. This loop will set the contents of `A` to be all 0. Then the second loop is encountered; the program will copy the array `A` (which is full of 0's) into GPU memory. Then, we will execute the second loop on the GPU. This will edit the GPU's copy of `A` to be full of 1's.\n",
    "\n",
    "At this point, we have two seperate copies of `A`. The CPU copy is full of 0's, and the GPU copy is full of 1's. Now, after the parallel region finishes, the program will copy `A` back from the GPU to the CPU. After this copy, both the CPU and the GPU will contain a copy of `A` that contains all 1's. The GPU copy of `A` will then be deallocated.\n",
    "\n",
    "This image offers another step-by-step example of using the copy clause.\n",
    "\n",
    "![copy_step_by_step](../images/copy_step_by_step.png)\n",
    "\n",
    "We are also able to copy multiple arrays at once by using the following syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop copy(A[0:N], B[0:N])\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = B[i];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Shaping\n",
    "\n",
    "The shape of the array specifies how much data needs to be transferred. Let's look at an example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop copy(A[0:N])\n",
    "for( int i = 0; i < N; i++ )\n",
    "{\n",
    "    A[i] = 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Focusing specifically on the `copy(A[0:N])`, the shape of the array is defined within the brackets. The syntax for array shape is **[starting_index:size]**. This means that (in the code example) we are copying data from array `A`, starting at index 0 (the start of the array), and copying N elements (which is most likely the length of the entire array).\n",
    "\n",
    "We are also able to only copy a portion of the array:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop copy(A[1:N-2])\n",
    "```\n",
    "\n",
    "This would copy all of the elements of **A** except for the first, and last element.\n",
    "\n",
    "Lastly, if you do not specify a starting index, 0 is assumed. This means that\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop copy(A[0:N])\n",
    "```\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop copy(A[:N])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Data Clauses in our Laplace Code\n",
    "\n",
    "Add `copy` data clause to our laplace code by selecting the following links:\n",
    "\n",
    "[jacobi.c](jacobi.c)  \n",
    "[laplace2d.c](laplace2d.c)  \n",
    "\n",
    "Then, when you are ready, you may run the code by running the following script. It may not be intuitively obvious yet, but we are expecting the code to perform very poorly. For this reason, we are running our GPU code on a **significantly smaller input size**. If you were to run the GPU code on the full sized input, it will take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "main:\n",
      "     60, Generating copyout(Anew[:m*n]) [if not already present]\n",
      "         Generating copyin(A[:m*n]) [if not already present]\n",
      "laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,m,n)\n",
      "         Generating NVIDIA GPU code\n",
      "         38, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "             Generating reduction(max:error)\n",
      "         40,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "swap:\n",
      "     51, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,n,m)\n",
      "         Generating NVIDIA GPU code\n",
      "         53, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         55,   /* blockIdx.x threadIdx.x collapsed */\n",
      "Jacobi relaxation Calculation: 1024 x 1024 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.742219 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -acc -gpu=ccnative -Minfo=accel -o laplace_data_clauses jacobi.c laplace2d.c && ./laplace_data_clauses 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure about your answer, you can view the solution [here.](solutions/basic_data/laplace2d.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Compiling GPU Code\n",
    "\n",
    "Let's execute the cell below to display information about the GPUs running on the server by running the `nvaccelinfo` command, which ships with the NVIDIA HPC compiler that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA Driver Version:           12060\n",
      "NVRM version:                  NVIDIA UNIX Open Kernel Module for aarch64  560.35.03  Release Build  (dvs-builder@U16-A24-2-4)  Fri Aug \n",
      "16 21:35:50 UTC 2024\n",
      "\n",
      "Device Number:                 0\n",
      "Device Name:                   NVIDIA GH200 120GB\n",
      "Device Revision Number:        9.0\n",
      "Global Memory Size:            102005473280\n",
      "Number of Multiprocessors:     132\n",
      "Concurrent Copy and Execution: Yes\n",
      "Total Constant Memory:         65536\n",
      "Total Shared Memory per Block: 49152\n",
      "Registers per Block:           65536\n",
      "Warp Size:                     32\n",
      "Maximum Threads per Block:     1024\n",
      "Maximum Block Dimensions:      1024, 1024, 64\n",
      "Maximum Grid Dimensions:       2147483647 x 65535 x 65535\n",
      "Maximum Memory Pitch:          2147483647B\n",
      "Texture Alignment:             512B\n",
      "Clock Rate:                    1980 MHz\n",
      "Execution Timeout:             No\n",
      "Integrated Device:             No\n",
      "Can Map Host Memory:           Yes\n",
      "Compute Mode:                  default\n",
      "Concurrent Kernels:            Yes\n",
      "ECC Enabled:                   Yes\n",
      "Memory Clock Rate:             2619 MHz\n",
      "Memory Bus Width:              6144 bits\n",
      "L2 Cache Size:                 62914560 bytes\n",
      "Max Threads Per SMP:           2048\n",
      "Async Engines:                 3\n",
      "Unified Addressing:            Yes\n",
      "Managed Memory:                Yes\n",
      "Concurrent Managed Memory:     Yes\n",
      "Preemption Supported:          Yes\n",
      "Cooperative Launch:            Yes\n",
      "Cluster Launch:                Yes\n",
      "Unified Function Pointers:     Yes\n",
      "Unified Memory:                ATS\n",
      "Memory Models Flags:           -gpu=mem:separate, -gpu=mem:managed, -gpu=mem:unified\n",
      "Default Target:                cc90\n",
      "\n",
      "Device Number:                 1\n",
      "Device Name:                   NVIDIA GH200 120GB\n",
      "Device Revision Number:        9.0\n",
      "Global Memory Size:            102005473280\n",
      "Number of Multiprocessors:     132\n",
      "Concurrent Copy and Execution: Yes\n",
      "Total Constant Memory:         65536\n",
      "Total Shared Memory per Block: 49152\n",
      "Registers per Block:           65536\n",
      "Warp Size:                     32\n",
      "Maximum Threads per Block:     1024\n",
      "Maximum Block Dimensions:      1024, 1024, 64\n",
      "Maximum Grid Dimensions:       2147483647 x 65535 x 65535\n",
      "Maximum Memory Pitch:          2147483647B\n",
      "Texture Alignment:             512B\n",
      "Clock Rate:                    1980 MHz\n",
      "Execution Timeout:             No\n",
      "Integrated Device:             No\n",
      "Can Map Host Memory:           Yes\n",
      "Compute Mode:                  default\n",
      "Concurrent Kernels:            Yes\n",
      "ECC Enabled:                   Yes\n",
      "Memory Clock Rate:             2619 MHz\n",
      "Memory Bus Width:              6144 bits\n",
      "L2 Cache Size:                 62914560 bytes\n",
      "Max Threads Per SMP:           2048\n",
      "Async Engines:                 3\n",
      "Unified Addressing:            Yes\n",
      "Managed Memory:                Yes\n",
      "Concurrent Managed Memory:     Yes\n",
      "Preemption Supported:          Yes\n",
      "Cooperative Launch:            Yes\n",
      "Cluster Launch:                Yes\n",
      "Unified Function Pointers:     Yes\n",
      "Unified Memory:                ATS\n",
      "Memory Models Flags:           -gpu=mem:separate, -gpu=mem:managed, -gpu=mem:unified\n",
      "Default Target:                cc90\n"
     ]
    }
   ],
   "source": [
    "!nvaccelinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information contained here, however, we are only going to focus on two points.\n",
    "\n",
    "**Managed Memory:** will tell us whether or not our GPU supports CUDA managed memory. We will cover managed memory a little bit later in the lab.\n",
    "\n",
    "**Compiler Option:** tells us which target to compiler for. Ealier we were using a `-gpu=multicore` flag for our multicore code. Now, to compile for our specific GPU, we will replace it with `-gpu=ccnative`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Profiling GPU Code\n",
    "\n",
    "In order to understand why our program is performing so poorly, we should consult our profiler. As stated previously, if we run our program with the default 4096x4096 array, the program will take several minutes to run. I recommend that you reduce the size. Try \"1024 1024\" as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA tracing has been automatically enabled since it is a prerequisite for tracing OpenACC.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 1024 x 1024 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.606536 s\n",
      "Generating '/tmp/nsys-report-1eb5.qdstrm'\n",
      "[1/7] [========================100%] laplace_data_clauses.nsys-rep\n",
      "[2/7] [========================100%] laplace_data_clauses.sqlite\n",
      "[3/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)          Name        \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------\n",
      "     72.5         44077792       6002    7343.9    4656.0      1120   1417728      23419.8  cuStreamSynchronize \n",
      "     15.0          9126912       1001    9117.8    8640.0      7872     62304       4211.1  cuMemcpyDtoHAsync_v2\n",
      "      9.4          5740032       3000    1913.3    1792.0      1280     19168        745.7  cuLaunchKernel      \n",
      "      1.9          1157024       1000    1157.0    1056.0       864      8416        538.5  cuMemsetD32Async    \n",
      "      0.6           351264          5   70252.8   99840.0      1952    128352      63030.4  cuMemAlloc_v2       \n",
      "      0.3           175488          1  175488.0  175488.0    175488    175488          0.0  cuMemAllocHost_v2   \n",
      "      0.3           156096          1  156096.0  156096.0    156096    156096          0.0  cuModuleLoadDataEx  \n",
      "      0.1            43680          1   43680.0   43680.0     43680     43680          0.0  cuMemcpyHtoDAsync_v2\n",
      "      0.0             1376          3     458.7     352.0       160       864        363.9  cuCtxSetCurrent     \n",
      "      0.0             1152          1    1152.0    1152.0      1152      1152          0.0  cuInit              \n",
      "\n",
      "[4/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                 Name                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------\n",
      "     42.0         10653664       1000   10653.7   10656.0     10496     11488         54.1  _11laplace2d_c_calcNext_36_gpu     \n",
      "     31.8          8063553       1000    8063.6    8032.0      7744      8544        119.4  _11laplace2d_c_calcNext_36_gpu__red\n",
      "     26.1          6625440       1000    6625.4    6624.0      6560      6912         24.5  _11laplace2d_c_swap_51_gpu         \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     61.3          1428416   1001    1427.0    1376.0      1344     52992       1633.4  [CUDA memcpy Device-to-Host]\n",
      "     37.6           875520   1000     875.5     864.0       864      1472         35.0  [CUDA memset]               \n",
      "      1.1            24608      1   24608.0   24608.0     24608     24608          0.0  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "      8.397   1001     0.008     0.000     0.000     8.389        0.265  [CUDA memcpy Device-to-Host]\n",
      "      8.389      1     8.389     8.389     8.389     8.389        0.000  [CUDA memcpy Host-to-Device]\n",
      "      0.008   1000     0.000     0.000     0.000     0.000        0.000  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'openacc_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                Name              \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------\n",
      "     22.7         30774624       3000   10258.2    5760.0      1408   1418976      27216.7  Wait@laplace2d.c:36             \n",
      "     20.2         27386464       1000   27386.5   27008.0     26336     89696       2827.6  Compute Construct@laplace2d.c:36\n",
      "     11.3         15290752       1000   15290.8   14016.0     13376    989760      30979.6  Compute Construct@laplace2d.c:51\n",
      "     10.2         13767648       2000    6883.8    8416.0      1408    986144      22547.1  Wait@laplace2d.c:51             \n",
      "      9.5         12858272       2000    6429.1   10864.0       416     69184       6857.0  Exit Data@laplace2d.c:36        \n",
      "      9.1         12283840       2000    6141.9    7968.0      1984   1422304      31901.5  Enter Data@laplace2d.c:36       \n",
      "      7.0          9455296       1000    9455.3    9024.0      8256     62880       3899.9  Enqueue Download@laplace2d.c:46 \n",
      "      3.4          4555520       2000    2277.8    2176.0      1664     20352        774.7  Enqueue Launch@laplace2d.c:36   \n",
      "      1.8          2396032       1000    2396.0    2272.0      1760     10976        785.6  Enqueue Launch@laplace2d.c:51   \n",
      "      1.7          2268896       1000    2268.9    2112.0      2016     57888       2416.0  Enter Data@laplace2d.c:51       \n",
      "      1.2          1606528       1000    1606.5    1536.0      1472     51360       1587.6  Wait@laplace2d.c:46             \n",
      "      1.1          1542496       1000    1542.5    1408.0      1184      8800        608.4  Enqueue Upload@laplace2d.c:36   \n",
      "      0.4           496672       1000     496.7     480.0       416      3840        156.2  Exit Data@laplace2d.c:51        \n",
      "      0.3           368192          1  368192.0  368192.0    368192    368192          0.0  Enter Data@jacobi.c:60          \n",
      "      0.1           197408          1  197408.0  197408.0    197408    197408          0.0  Device Init@jacobi.c:60         \n",
      "      0.1            68352          1   68352.0   68352.0     68352     68352          0.0  Exit Data@jacobi.c:60           \n",
      "      0.0            63488          1   63488.0   63488.0     63488     63488          0.0  Enqueue Download@jacobi.c:71    \n",
      "      0.0            48320          1   48320.0   48320.0     48320     48320          0.0  Enqueue Upload@jacobi.c:60      \n",
      "      0.0             4064          1    4064.0    4064.0      4064      4064          0.0  Wait@jacobi.c:60                \n",
      "      0.0             1600          1    1600.0    1600.0      1600      1600          0.0  Wait@jacobi.c:71                \n",
      "      0.0                0          2       0.0       0.0         0         0          0.0  Alloc@jacobi.c:60               \n",
      "      0.0                0          1       0.0       0.0         0         0          0.0  Alloc@laplace2d.c:36            \n",
      "      0.0                0          2       0.0       0.0         0         0          0.0  Create@jacobi.c:60              \n",
      "      0.0                0       1000       0.0       0.0         0         0          0.0  Create@laplace2d.c:36           \n",
      "      0.0                0          2       0.0       0.0         0         0          0.0  Delete@jacobi.c:71              \n",
      "      0.0                0       1000       0.0       0.0         0         0          0.0  Delete@laplace2d.c:46           \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/OpenACC/module4/English/C/laplace_data_clauses.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/OpenACC/module4/English/C/laplace_data_clauses.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace_data_clauses ./laplace_data_clauses 1024 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_data_clauses.nsys-rep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport).\n",
    "\n",
    "This is the view that you should see once you open the profiler report via GUI.\n",
    "\n",
    "![data_clause1.PNG](../images/data_clause1.png)\n",
    "\n",
    "We can see that our \"timeline\" has a lot going on. Feel free to explore the profile at this point. It will help to zoom in, so that you can better see the information.\n",
    "\n",
    "![data_clause2.PNG](../images/data_clause2.png)\n",
    "\n",
    "Upon zooming in, we get a much better idea of what is happening inside of our program. Zoom in on a single iteration of the while loop and see where each of `calcNext` and `swap` is called. You can also see a lot of space between them. It may be obvious now why our program is performing so poorly. The amount of time that our program is transferring data (as seen in the MemCpy timelines) is far greater than the time it takes running our computational functions `calcNext` and `swap`. In order to improve our performance, we need to minimize these data transfers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Managed Memory\n",
    "\n",
    "![managed_memory.png](../images/cuda-unified-memory.svg)  \n",
    "\n",
    "As with many things in OpenACC, we have the option to allow the compiler to handle memory management. We may be able to achieve better performance by managing the memory ourselves, however, allowing the compiler to use managed memory is very simple, and will achieve much better performance than our naive solution from earlier. We do not need to make any changes to our code to get managed memory working. Simply run the following script. Keep in mind that unlike earlier, we are now running our code with the full sized 4096x4096 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobi.c:\n",
      "main:\n",
      "     60, Generating copyout(Anew[:m*n]) [if not already present]\n",
      "         Generating copyin(A[:m*n]) [if not already present]\n",
      "laplace2d.c:\n",
      "calcNext:\n",
      "     36, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,m,n)\n",
      "         Generating NVIDIA GPU code\n",
      "         38, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "             Generating reduction(max:error)\n",
      "         40,   /* blockIdx.x threadIdx.x collapsed */\n",
      "     36, Generating implicit copy(error) [if not already present]\n",
      "swap:\n",
      "     51, Generating present(A[:],Anew[:])\n",
      "         Generating implicit firstprivate(j,n,m)\n",
      "         Generating NVIDIA GPU code\n",
      "         53, #pragma acc loop gang, vector(128) collapse(2) /* blockIdx.x threadIdx.x */\n",
      "         55,   /* blockIdx.x threadIdx.x collapsed */\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.392625 s\n"
     ]
    }
   ],
   "source": [
    "!nvc -fast -acc -gpu=mem:managed -gpu=ccnative -Minfo=accel -o laplace_managed jacobi.c laplace2d.c && ./laplace_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Compiling with the Managed Memory Flag\n",
    "\n",
    "As long as the GPU supports managed memory (see [Optional: Compiling GPU Code](#Optional:-Compiling-GPU-Code) to learn how to check if your GPU supports it), all you need to do is add the managed option to our `-gpu` flag.\n",
    "\n",
    "`-gpu=men:managed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling our Managed Memory Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA tracing has been automatically enabled since it is a prerequisite for tracing OpenACC.\n",
      "Collecting data...\n",
      "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
      "    0, 0.250000\n",
      "  100, 0.002397\n",
      "  200, 0.001204\n",
      "  300, 0.000804\n",
      "  400, 0.000603\n",
      "  500, 0.000483\n",
      "  600, 0.000403\n",
      "  700, 0.000345\n",
      "  800, 0.000302\n",
      "  900, 0.000269\n",
      " total: 0.952221 s\n",
      "Generating '/tmp/nsys-report-3346.qdstrm'\n",
      "[1/7] [========================100%] laplace_managed.nsys-rep\n",
      "[2/7] [========================100%] laplace_managed.sqlite\n",
      "[3/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)          Name        \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------\n",
      "     95.2        392918304       6002   65464.6    3856.0      1152  13952288     294756.2  cuStreamSynchronize \n",
      "      2.5         10301504       1001   10291.2    9344.0      7968    807232      25246.8  cuMemcpyDtoHAsync_v2\n",
      "      1.6          6558752       3000    2186.3    1952.0      1376     23008       1058.3  cuLaunchKernel      \n",
      "      0.4          1450144       1000    1450.1    1280.0       864     27200       1294.6  cuMemsetD32Async    \n",
      "      0.2           777408          5  155481.6  129408.0      1952    329440     160515.5  cuMemAlloc_v2       \n",
      "      0.1           347264          1  347264.0  347264.0    347264    347264          0.0  cuMemcpyHtoDAsync_v2\n",
      "      0.0           173696          1  173696.0  173696.0    173696    173696          0.0  cuMemAllocHost_v2   \n",
      "      0.0           163040          1  163040.0  163040.0    163040    163040          0.0  cuModuleLoadDataEx  \n",
      "      0.0             1472          3     490.7     384.0       128       960        426.1  cuCtxSetCurrent     \n",
      "      0.0             1120          1    1120.0    1120.0      1120      1120          0.0  cuInit              \n",
      "\n",
      "[4/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                 Name                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------\n",
      "     40.2        131970815       1000  131970.8  132032.0    129600    134304        737.8  _11laplace2d_c_calcNext_36_gpu     \n",
      "     30.6        100388255       1000  100388.3  100416.0     99232    101216        300.6  _11laplace2d_c_swap_51_gpu         \n",
      "     29.2         95813375       1000   95813.4   95872.0     94208     97376        559.8  _11laplace2d_c_calcNext_36_gpu__red\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     65.7          2171552   1001    2169.4    1376.0      1344    796032      25116.9  [CUDA memcpy Device-to-Host]\n",
      "     24.3           804320   1000     804.3     800.0       768      1152         23.3  [CUDA memset]               \n",
      "      9.9           328064      1  328064.0  328064.0    328064    328064          0.0  [CUDA memcpy Host-to-Device]\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    134.226   1001     0.134     0.000     0.000   134.218        4.242  [CUDA memcpy Device-to-Host]\n",
      "    134.218      1   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Host-to-Device]\n",
      "      0.008   1000     0.000     0.000     0.000     0.000        0.000  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'openacc_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                Name              \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  --------------------------------\n",
      "     32.8        276632000       3000    92210.7     5696.0      1440  13953568     395600.2  Wait@laplace2d.c:36             \n",
      "     32.6        275095232       1000   275095.2   236736.0    232896  13959520     650253.8  Compute Construct@laplace2d.c:36\n",
      "     14.1        119024640       1000   119024.6   108128.0    106464   5285312     198434.6  Compute Construct@laplace2d.c:51\n",
      "     13.9        117052256       2000    58526.1    54448.0      1440   5281120     151388.4  Wait@laplace2d.c:51             \n",
      "      1.8         15568384       2000     7784.2     7872.0      2016   3171744      74242.4  Enter Data@laplace2d.c:36       \n",
      "      1.6         13561280       2000     6780.6     6784.0       416     47776       6419.4  Exit Data@laplace2d.c:36        \n",
      "      1.2          9978176       1000     9978.2     9760.0      8352     35744       1626.3  Enqueue Download@laplace2d.c:46 \n",
      "      0.6          5083008       2000     2541.5     2304.0      1728     20544       1097.4  Enqueue Launch@laplace2d.c:36   \n",
      "      0.3          2749920       1000     2749.9     2496.0      1792     23424       1183.6  Enqueue Launch@laplace2d.c:51   \n",
      "      0.3          2210688       1000     2210.7     2144.0      2016      7872        318.4  Enter Data@laplace2d.c:51       \n",
      "      0.2          1879360       1000     1879.4     1664.0      1152     27872       1397.6  Enqueue Upload@laplace2d.c:36   \n",
      "      0.2          1583648       1000     1583.6     1536.0      1472     14496        436.9  Wait@laplace2d.c:46             \n",
      "      0.1          1110400          1  1110400.0  1110400.0   1110400   1110400          0.0  Enter Data@jacobi.c:60          \n",
      "      0.1           814304          1   814304.0   814304.0    814304    814304          0.0  Exit Data@jacobi.c:60           \n",
      "      0.1           809024          1   809024.0   809024.0    809024    809024          0.0  Enqueue Download@jacobi.c:71    \n",
      "      0.1           563936       1000      563.9      480.0       448      4000        291.6  Exit Data@laplace2d.c:51        \n",
      "      0.0           351872          1   351872.0   351872.0    351872    351872          0.0  Enqueue Upload@jacobi.c:60      \n",
      "      0.0           207808          1   207808.0   207808.0    207808    207808          0.0  Device Init@jacobi.c:60         \n",
      "      0.0             4512          1     4512.0     4512.0      4512      4512          0.0  Wait@jacobi.c:60                \n",
      "      0.0             1600          1     1600.0     1600.0      1600      1600          0.0  Wait@jacobi.c:71                \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Alloc@jacobi.c:60               \n",
      "      0.0                0          1        0.0        0.0         0         0          0.0  Alloc@laplace2d.c:36            \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Create@jacobi.c:60              \n",
      "      0.0                0       1000        0.0        0.0         0         0          0.0  Create@laplace2d.c:36           \n",
      "      0.0                0          2        0.0        0.0         0         0          0.0  Delete@jacobi.c:71              \n",
      "      0.0                0       1000        0.0        0.0         0         0          0.0  Delete@laplace2d.c:46           \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/OpenACC/module4/English/C/laplace_managed.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/OpenACC/module4/English/C/laplace_managed.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t openacc --stats=true --force-overwrite true -o laplace_managed ./laplace_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the profiler's report. Once the profiling run has completed, download and save the report file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](laplace_managed.nsys-rep) (choose *save link as*), and open it via the GUI. To view the profiler report locally, please see the section on [How to view the report](../../../module2/English/C/README.ipynb#viewreport).\n",
    "\n",
    "![managed1.PNG](../images/managed1.png)\n",
    "\n",
    "Feel free to explore the profile at this point. Then, when you're ready, let's zoom in.\n",
    "\n",
    "![managed2.PNG](../images/managed2.png)\n",
    "\n",
    "We can see that our compute regions (our `calcNext` and `swap` function calls) are much closer together now. There is significantly less data transfer happening between them. By using managed memory, the compiler was able to avoid the need to transfer data back and forth between the CPU and the GPU. In the next module, we will learn how to do this manually (which will boost the performance by a little bit), but for now, it is sufficient to use managed memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have learned how to run our code on a GPU using managed memory. We also experimented a little bit with managing the data ourselves, but that didn't work out as well as we had hoped. In the next module, we will expand on these data concepts and learn the proper way to manage our data, and will no longer need to rely on the compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Task\n",
    "\n",
    "1. If you would like some additional lessons on using OpenACC to parallelize our code, there is an Introduction to OpenACC video series available from the OpenACC YouTube page. The third and fourth video in the series covers a lot of the content that was covered in this lab.  \n",
    "[Introduction to Parallel Programming with OpenACC - Part 3](https://youtu.be/Pcc3O6h-YPE)  \n",
    "[Introduction to Parallel Programming with OpenACC - Part 4](https://youtu.be/atXtVCHq8iw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommended you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f openacc_files.zip\n",
    "zip -r openacc_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](openacc_files.zip) and choose *save link as*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17aa471",
   "metadata": {},
   "source": [
    "# Solveur de Jacobi : Modèles de programmation Multi-GPU\n",
    "Ce notebook présente 12 versions progressives d’un solveur de Jacobi 2D. Chaque section explique le modèle ou l’optimisation, compile la version, exécute et collecte les métriques Nsight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506cee8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Modules Spack à charger\n",
    "\n",
    "Avant de compiler ou d’exécuter les différentes étapes, il est recommandé de charger les modules nécessaires via Spack. Par exemple :\n",
    "\n",
    "```bash\n",
    "spack load cuda@12.6\n",
    "spack load nvidia-nsight-systems@2024.6.1\n",
    "spack load nvhpc@24.11 \n",
    "spack load cudnn@9.2.0.82-12\n",
    "```\n",
    "\n",
    "Adaptez la version de chaque module selon la configuration de votre cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8dae0a",
   "metadata": {},
   "source": [
    "## etape1_cpu\n",
    "**Description :** Solveur Jacobi CPU de base : implémentation mono-thread. Utile pour valider la correction et les petites tailles de problème ; met en évidence la limite de calcul CPU.\n",
    "\n",
    "**Intérêt :** Baseline : évalue la limite CPU pour établir une référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e7495f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f main\n",
      "gcc -O2 -o main main.c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d75f86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=2550785): \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-709b.qdstrm'\n",
      "[1/8] [========================100%] main.nsys-rep\n",
      "[2/8] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain OS Runtime trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA kernel data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "nsys profile --stats=true --force-overwrite true -o main ./main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2f1db",
   "metadata": {},
   "source": [
    "## etape2_cpu_gpu\n",
    "**Description :** 1 CPU + 1 GPU + 1 stream : le CPU pilote le GPU via un unique stream CUDA. Le calcul Jacobi est entièrement délégué au GPU, le CPU ne fait que l’orchestration.\n",
    "\n",
    "**Intérêt :** Met en évidence l'écart de performance CPU vs GPU lorsque la grille est suffisamment grande, dans un contexte réaliste d’utilisation d’un seul GPU et d’un seul stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70bb1",
   "metadata": {},
   "source": [
    "### Compilation et exécution (1 CPU + 1 GPU + 1 stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61559afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45d30d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape2_cpu_gpu (1CPU + 1GPU + 1stream)\n",
      "GPU time: 48.874808 seconds\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-a648.qdstrm'\n",
      "[1/6] [========================100%] main.nsys-rep\n",
      "[2/6] [========================100%] main.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)       Med (ns)     Min (ns)    Max (ns)     StdDev (ns)            Name         \n",
      " --------  ---------------  ---------  -------------  -------------  ---------  -----------  -------------  ----------------------\n",
      "     88.2      42956795360          3  14318931786.7  20899479424.0   46286208  22011029728  12372962251.6  cudaMemcpyAsync       \n",
      "     11.4       5554665760       1001      5549116.6      5513056.0       1824     10656032       460412.7  cudaStreamSynchronize \n",
      "      0.3        155750016          1    155750016.0    155750016.0  155750016    155750016            0.0  cudaStreamCreate      \n",
      "      0.0         23931008          2     11965504.0     11965504.0   11949984     11981024        21948.6  cudaMalloc            \n",
      "      0.0         13004960          2      6502480.0      6502480.0    6389056      6615904       160405.8  cudaFree              \n",
      "      0.0          3701920       1000         3701.9         2912.0       2080       627488        19759.5  cudaLaunchKernel      \n",
      "      0.0            22208          1        22208.0        22208.0      22208        22208            0.0  cudaStreamDestroy     \n",
      "      0.0             1664          1         1664.0         1664.0       1664         1664            0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  --------------------------------------\n",
      "    100.0       5511111785       1000  5511111.8  5507391.5   5498080   5600928      10108.3  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  -------------  -------------  -----------  -----------  -----------  ----------------------------\n",
      "     99.9      42910038397      2  21455019198.5  21455019198.5  20899407783  22010630614  785753199.2  [CUDA memcpy Host-to-Device]\n",
      "      0.1         46241377      1     46241377.0     46241377.0     46241377     46241377          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  16384.000      2  8192.000  8192.000  8192.000  8192.000        0.000  [CUDA memcpy Host-to-Device]\n",
      "   8192.000      1  8192.000  8192.000  8192.000  8192.000        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "nsys profile -t cuda --stats=true --force-overwrite true -o main ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20823ca",
   "metadata": {},
   "source": [
    "## etape3_mpi_gpus\n",
    "**Description :** MPI + GPUs : domaine réparti sur plusieurs rangs MPI, chacun pilotant un GPU. Illustrations des défis de mise à l'échelle multi-nœuds et des communications inter-rangs.\n",
    "\n",
    "**Intérêt :** Test de montée en charge inter-nœuds et coût MPI sur cluster multi-GPU.\n",
    "\n",
    "**Step :** \n",
    "- Initialiser MPI, déterminer le rang et le nombre de processus.\n",
    "- Associer chaque rang à un GPU différent.\n",
    "- Diviser la grille entre les rangs (découpage 1D vertical).\n",
    "- Gérer les échanges d’halos entre rangs voisins avec MPI_Sendrecv.\n",
    "- Synchroniser les échanges à chaque itération.\n",
    "- Nettoyer MPI à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0777a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -lcudart -lmpi -lnccl -lstdc++ -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape3_mpi_gpus\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "173bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'étape 3 : MPI + multi-GPU\n",
      "Grille de taille 4096 x 4096, T = 1000 itérations\n",
      "Terminé etape3_mpi_gpus (MPI + multi-GPU) rank 0/4\n",
      "Max GPU time: 0.580814 seconds rank 0/4\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-bbef.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------\n",
      "     84.6        782673472      12012   65157.6    8192.0      3136  90009984    2251672.8  cudaMemcpy            \n",
      "     12.9        119267680       4000   29816.9   29792.0     19584     64736       1799.0  cudaDeviceSynchronize \n",
      "      1.5         13830432       4000    3457.6    2816.0      2016    502304      13913.4  cudaLaunchKernel      \n",
      "      0.6          5358016          8  669752.0  658880.0    568896    810272      72805.9  cudaFree              \n",
      "      0.4          3682240          8  460280.0  490320.0    222464    675776     158586.3  cudaMalloc            \n",
      "      0.1           505216       1652     305.8     224.0        32      3584        288.7  cuGetProcAddress_v2   \n",
      "      0.0            11360          4    2840.0    3008.0      2048      3296        567.3  cuInit                \n",
      "      0.0             2112          4     528.0     528.0       384       672        149.0  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         98161855       4000   24540.5   24512.0     21600     25568        294.3  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     98.0        712704282   6008  118625.9    2400.0      2208  89991006    3182013.4  [CUDA memcpy Host-to-Device]\n",
      "      2.0         14199424   6004    2365.0    1856.0      1664    815968      20141.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    464.912   6008     0.077     0.033     0.033    33.554        1.222  [CUDA memcpy Host-to-Device]\n",
      "    330.760   6004     0.055     0.033     0.033    33.554        0.865  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape3_mpi_gpus\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 4 ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f4634",
   "metadata": {},
   "source": [
    "### À partir de quelle taille de matrice le recouvrement communication/calcul devient-il rentable ?\n",
    "\n",
    "Le recouvrement (overlap) communication/calcul devient généralement rentable lorsque :\n",
    "- Le temps de communication (MPI + transferts host/device) devient significatif devant le temps de calcul local.\n",
    "- La partie du calcul qui peut être effectuée pendant la communication (hors bords) est suffisamment grande pour masquer la latence réseau.\n",
    "\n",
    "Pour une grille Jacobi 2D, la taille critique dépend :\n",
    "- De la bande passante et latence réseau,\n",
    "- Du nombre de rangs MPI,\n",
    "- De la rapidité des transferts CUDA Host/Device,\n",
    "- De la puissance du GPU.\n",
    "\n",
    "**Sur la plupart des clusters modernes, le recouvrement commence à être rentable pour des matrices de l’ordre de 16k×16k à 32k×32k (voire plus),** surtout si le nombre de rangs est élevé (≥4) et que la communication devient un vrai goulot d’étranglement.\n",
    "\n",
    "**Pour une matrice 8k×8k,** le calcul local reste souvent dominant, donc le surcoût du overlap (copies, synchronisations) peut masquer le gain.  \n",
    "**Essayez avec 16k×16k ou 32k×32k** pour voir un bénéfice, surtout si vous augmentez le nombre de rangs MPI (et donc la proportion de communication).\n",
    "\n",
    "**Résumé :**  \n",
    "- < 8k×8k : overlap rarement utile  \n",
    "- 16k×16k : commence à être intéressant  \n",
    "- 32k×32k et + : overlap souvent rentable, surtout sur cluster multi-nœuds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b520d22",
   "metadata": {},
   "source": [
    "## etape4_mpi_overlap\n",
    "**Description :** MPI + recouvrement : recouvrements des échanges d’halo non-bloquants avec le calcul Jacobi local. Réduit l'impact de la latence réseau.\n",
    "\n",
    "**Intérêt :** Cache la latence réseau en recouvrant communication et calcul local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8382b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -lcudart -lmpi -lnccl -lstdc++ -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape4_mpi_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be3815dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'itération Jacobi multi-GPU avec chevauchement des flux...\n",
      "Rank 3 terminé\n",
      "Rank 0 terminé\n",
      "Rank 2 terminé\n",
      "Rank 1 terminé\n",
      "Temps total (Jacobi multi-GPU overlap, 4 rangs): 0.081824 secondes\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-5681.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------\n",
      "     91.8        787002976      12008   65539.9    9232.0      2592  88237760    2253220.4  cudaMemcpyAsync       \n",
      "      4.4         37327456      16000    2333.0    1760.0      1280    153664       1893.1  cudaStreamSynchronize \n",
      "      2.5         21089536       4000    5272.4    4384.0      2848    819744      22815.9  cudaLaunchKernel      \n",
      "      0.6          5020416          8  627552.0  636688.0    384832    782912     130806.6  cudaFree              \n",
      "      0.4          3475744          4  868936.0  891504.0    782912    909824      58604.9  cudaMemcpy            \n",
      "      0.3          2399968          8  299996.0  301392.0    228544    386304      49745.0  cudaMalloc            \n",
      "      0.1           748384         12   62365.3   15472.0      3936    218816      79395.3  cudaStreamCreate      \n",
      "      0.1           455808       1652     275.9     224.0        32      4928        306.9  cuGetProcAddress_v2   \n",
      "      0.0           101408         12    8450.7    5392.0      4608     17216       5072.0  cudaStreamDestroy     \n",
      "      0.0            11904          4    2976.0    3088.0      2048      3680        688.8  cuInit                \n",
      "      0.0             2048          4     512.0     528.0       288       704        171.3  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                        Name                      \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ------------------------------------------------\n",
      "    100.0         98206588       4000   24551.6   24544.0     22720     25824        312.8  jacobi_kernel(double *, double *, int, int, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     97.8        713788787   6008  118806.4    2496.0      2208  88220509    3183761.4  [CUDA memcpy Host-to-Device]\n",
      "      2.2         15834048   6004    2637.2    2048.0      1856    895808      22039.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    465.240   6008     0.077     0.033     0.033    33.587        1.223  [CUDA memcpy Host-to-Device]\n",
      "    330.924   6004     0.055     0.033     0.033    33.587        0.866  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape4_mpi_overlap\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 4 ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86582246",
   "metadata": {},
   "source": [
    "## etape5_nccl\n",
    "**Description :** NCCL : utilisation de la NVIDIA Collective Communications Library pour les échanges GPU à GPU. Montre les gains via NVLink ou PCIe haute bande passante.\n",
    "\n",
    "**Intérêt :** Exploite automatiquement le topologie NVLink/PCIe pour des échanges GPU efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86536f",
   "metadata": {},
   "source": [
    "### Introduction à NCCL\n",
    "\n",
    "NCCL (NVIDIA Collective Communications Library) permet des communications collectives efficaces entre plusieurs GPU, en exploitant la topologie matérielle (NVLink, PCIe).  \n",
    "Dans un contexte Jacobi multi-GPU, NCCL peut être utilisé pour échanger les halos entre GPUs sans repasser par le CPU.\n",
    "\n",
    "**Exemple minimal d'utilisation de NCCL pour un échange entre deux GPU :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d614a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -lnccl -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape5_nccl\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape5_nccl\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "nsys profile -t cuda --stats=true --force-overwrite true -o main ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1f7c0",
   "metadata": {},
   "source": [
    "## etape6_nccl_overlap\n",
    "**Description :** NCCL + recouvrement : superposition des collectifs NCCL avec le calcul sur GPU, cachant le coût de communication.\n",
    "\n",
    "**Intérêt :** Essentiel à forte densité GPU pour maintenir les cœurs occupés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape6_nccl_overlap\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape6_nccl_overlap \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape6_nccl_overlap.csv ./main\n",
    "cat rapport_etape6_nccl_overlap.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3c561",
   "metadata": {},
   "source": [
    "## etape7_nccl_graphs\n",
    "**Description :** NCCL + CUDA Graphs : capture et relecture des séquences Jacobi/échange pour réduire le surcoût des lancements.\n",
    "\n",
    "**Intérêt :** Réduit l’overhead de lancement grâce aux CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65978918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape7_nccl_graphs.csv ./main\n",
    "cat rapport_etape7_nccl_graphs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4aaa2",
   "metadata": {},
   "source": [
    "## etape8_nvshmem\n",
    "**Description :** NVSHMEM : modèle PGAS à accès mémoire unilatéral GPU, simplifiant les mises à jour d’halo.\n",
    "\n",
    "**Intérêt :** Simplifie les échanges via modèle PGAS unilatéral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a25a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape8_nvshmem.csv ./main\n",
    "cat rapport_etape8_nvshmem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9b0f5",
   "metadata": {},
   "source": [
    "## etape9_nvshmem_lt\n",
    "**Description :** NVSHMEM + LTO : ajout de l’optimisation link-time pour inliner les fonctions critiques et réduire le coût des appels.\n",
    "\n",
    "**Intérêt :** Optimisation link-time pour inliner les sections critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape9_nvshmem_lt\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape9_nvshmem_lt\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape9_nvshmem_lt.csv ./main\n",
    "cat rapport_etape9_nvshmem_lt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bbaef",
   "metadata": {},
   "source": [
    "## etape10_vshmem_neighborhood_lto\n",
    "**Description :** vshmem neighborhood_sync + LTO : synchronisation fine-grain de voisinage et optimisations link-time O2.\n",
    "\n",
    "**Intérêt :** Synchronisation fine et LTO pour boucles serrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape10_vshmem_neighborhood_lto.csv ./main \n",
    "cat rapport_etape10_vshmem_neighborhood_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0e44",
   "metadata": {},
   "source": [
    "## etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "**Description :** Combinaison : NVSHMEM avec recouvrement, synchrone de voisinage, et LTO pour maximiser la concurrence.\n",
    "\n",
    "**Intérêt :** Combinaison des meilleures pratiques pour un binaire ultra-optimisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv ./main\n",
    "cat rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33006f2a",
   "metadata": {},
   "source": [
    "## etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1\n",
    "**Description :** Tuning étendu : paramètres ajustables (taille de tuile, ordre de boucles) et hooks de benchmark.\n",
    "\n",
    "**Intérêt :** Ajout de paramètres de tuning et hooks de benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c846b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv ./main\n",
    "cat rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

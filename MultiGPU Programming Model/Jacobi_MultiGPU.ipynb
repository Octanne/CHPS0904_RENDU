{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17aa471",
   "metadata": {},
   "source": [
    "# Solveur de Jacobi : Modèles de programmation Multi-GPU\n",
    "Ce notebook présente 12 versions progressives d’un solveur de Jacobi 2D. Chaque section explique le modèle ou l’optimisation, compile la version, exécute et collecte les métriques Nsight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506cee8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Modules Spack à charger\n",
    "\n",
    "Avant de compiler ou d’exécuter les différentes étapes, il est recommandé de charger les modules nécessaires via Spack. Par exemple :\n",
    "\n",
    "```bash\n",
    "spack load cuda@12.6\n",
    "spack load nvidia-nsight-systems@2024.6.1\n",
    "spack load nvhpc@24.11 \n",
    "spack load cudnn@9.2.0.82-12\n",
    "```\n",
    "\n",
    "Adaptez la version de chaque module selon la configuration de votre cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8dae0a",
   "metadata": {},
   "source": [
    "## etape1_cpu\n",
    "**Description :** Solveur Jacobi CPU de base : implémentation mono-thread. Utile pour valider la correction et les petites tailles de problème ; met en évidence la limite de calcul CPU.\n",
    "\n",
    "**Intérêt :** Baseline : évalue la limite CPU pour établir une référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e7495f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f main\n",
      "gcc -O2 -o main main.c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=2550785): \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-709b.qdstrm'\n",
      "[1/8] [========================100%] main.nsys-rep\n",
      "[2/8] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain OS Runtime trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA kernel data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "nsys profile --stats=true --force-overwrite true -o main ./main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2f1db",
   "metadata": {},
   "source": [
    "## etape2_cpu_gpu\n",
    "**Description :** 1 CPU + 1 GPU + 1 stream : le CPU pilote le GPU via un unique stream CUDA. Le calcul Jacobi est entièrement délégué au GPU, le CPU ne fait que l’orchestration.\n",
    "\n",
    "**Intérêt :** Met en évidence l'écart de performance CPU vs GPU lorsque la grille est suffisamment grande, dans un contexte réaliste d’utilisation d’un seul GPU et d’un seul stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70bb1",
   "metadata": {},
   "source": [
    "### Compilation et exécution (1 CPU + 1 GPU + 1 stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61559afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45d30d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape2_cpu_gpu (1CPU + 1GPU + 1stream)\n",
      "GPU time: 48.874808 seconds\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-a648.qdstrm'\n",
      "[1/6] [========================100%] main.nsys-rep\n",
      "[2/6] [========================100%] main.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)       Med (ns)     Min (ns)    Max (ns)     StdDev (ns)            Name         \n",
      " --------  ---------------  ---------  -------------  -------------  ---------  -----------  -------------  ----------------------\n",
      "     88.2      42956795360          3  14318931786.7  20899479424.0   46286208  22011029728  12372962251.6  cudaMemcpyAsync       \n",
      "     11.4       5554665760       1001      5549116.6      5513056.0       1824     10656032       460412.7  cudaStreamSynchronize \n",
      "      0.3        155750016          1    155750016.0    155750016.0  155750016    155750016            0.0  cudaStreamCreate      \n",
      "      0.0         23931008          2     11965504.0     11965504.0   11949984     11981024        21948.6  cudaMalloc            \n",
      "      0.0         13004960          2      6502480.0      6502480.0    6389056      6615904       160405.8  cudaFree              \n",
      "      0.0          3701920       1000         3701.9         2912.0       2080       627488        19759.5  cudaLaunchKernel      \n",
      "      0.0            22208          1        22208.0        22208.0      22208        22208            0.0  cudaStreamDestroy     \n",
      "      0.0             1664          1         1664.0         1664.0       1664         1664            0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  --------------------------------------\n",
      "    100.0       5511111785       1000  5511111.8  5507391.5   5498080   5600928      10108.3  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  -------------  -------------  -----------  -----------  -----------  ----------------------------\n",
      "     99.9      42910038397      2  21455019198.5  21455019198.5  20899407783  22010630614  785753199.2  [CUDA memcpy Host-to-Device]\n",
      "      0.1         46241377      1     46241377.0     46241377.0     46241377     46241377          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  16384.000      2  8192.000  8192.000  8192.000  8192.000        0.000  [CUDA memcpy Host-to-Device]\n",
      "   8192.000      1  8192.000  8192.000  8192.000  8192.000        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "nsys profile -t cuda --stats=true --force-overwrite true -o main ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20823ca",
   "metadata": {},
   "source": [
    "## etape3_mpi_gpus\n",
    "**Description :** MPI + GPUs : domaine réparti sur plusieurs rangs MPI, chacun pilotant un GPU. Illustrations des défis de mise à l'échelle multi-nœuds et des communications inter-rangs.\n",
    "\n",
    "**Intérêt :** Test de montée en charge inter-nœuds et coût MPI sur cluster multi-GPU.\n",
    "\n",
    "**Step :** \n",
    "- Initialiser MPI, déterminer le rang et le nombre de processus.\n",
    "- Associer chaque rang à un GPU différent.\n",
    "- Diviser la grille entre les rangs (découpage 1D vertical).\n",
    "- Gérer les échanges d’halos entre rangs voisins avec MPI_Sendrecv.\n",
    "- Synchroniser les échanges à chaque itération.\n",
    "- Nettoyer MPI à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0777a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -Xcompiler \"-fopenmp\" -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.cu -o jacobi -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -lstdc++ \\\n",
      "\t-Xlinker --no-as-needed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape3_mpi_gpus\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 1000 iters in 7.20935s, norm=0.309572\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-54f1.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------\n",
      "     97.4        822638144      10004   82230.9   12288.0      3328    432224     142690.9  cudaMemcpy            \n",
      "      2.3         19455296       2000    9727.6    9472.0      7232    361248      10730.9  cudaLaunchKernel      \n",
      "      0.1          1134080          4  283520.0  287232.0    164512    395104     125392.3  cudaFree              \n",
      "      0.1           776288          4  194072.0  183776.0    153568    255168      45801.7  cudaMalloc            \n",
      "      0.0           151360        826     183.2      96.0        32      3424        236.9  cuGetProcAddress_v2   \n",
      "      0.0            75744          4   18936.0   14864.0      5248     40768      17099.7  cudaMemset            \n",
      "      0.0             7232          2    3616.0    3616.0      3040      4192        814.6  cuInit                \n",
      "      0.0             2016          2    1008.0    1008.0       992      1024         22.6  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                      \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------------------\n",
      "    100.0         79357824       2000   39678.9   39584.0     39072     40544        296.8  jacobi_kernel(const float *, float *, int, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     98.9        710434847   6000  118405.8    4032.0      1600    392512     164707.7  [CUDA memcpy Device-to-Host]\n",
      "      1.0          7532832   4004    1881.3    1504.0      1408    385504       8954.5  [CUDA memcpy Host-to-Device]\n",
      "      0.0            38976      4    9744.0    9792.0      8672     10720       1062.4  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  67207.168   6000    11.201     0.016     0.016    33.571       15.819  [CUDA memcpy Device-to-Host]\n",
      "    199.819   4004     0.050     0.016     0.016    33.571        1.060  [CUDA memcpy Host-to-Device]\n",
      "    134.283      4    33.571    33.571    33.571    33.571        0.000  [CUDA memset]               \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape3_mpi_gpus\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 2 ./jacobi 1000 4096 4096 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f4634",
   "metadata": {},
   "source": [
    "### À partir de quelle taille de matrice le recouvrement communication/calcul devient-il rentable ?\n",
    "\n",
    "Le recouvrement (overlap) communication/calcul devient généralement rentable lorsque :\n",
    "- Le temps de communication (MPI + transferts host/device) devient significatif devant le temps de calcul local.\n",
    "- La partie du calcul qui peut être effectuée pendant la communication (hors bords) est suffisamment grande pour masquer la latence réseau.\n",
    "\n",
    "Pour une grille Jacobi 2D, la taille critique dépend :\n",
    "- De la bande passante et latence réseau,\n",
    "- Du nombre de rangs MPI,\n",
    "- De la rapidité des transferts CUDA Host/Device,\n",
    "- De la puissance du GPU.\n",
    "\n",
    "**Sur la plupart des clusters modernes, le recouvrement commence à être rentable pour des matrices de l’ordre de 16k×16k à 32k×32k (voire plus),** surtout si le nombre de rangs est élevé (≥4) et que la communication devient un vrai goulot d’étranglement.\n",
    "\n",
    "**Pour une matrice 8k×8k,** le calcul local reste souvent dominant, donc le surcoût du overlap (copies, synchronisations) peut masquer le gain.  \n",
    "**Essayez avec 16k×16k ou 32k×32k** pour voir un bénéfice, surtout si vous augmentez le nombre de rangs MPI (et donc la proportion de communication).\n",
    "\n",
    "**Résumé :**  \n",
    "- < 8k×8k : overlap rarement utile  \n",
    "- 16k×16k : commence à être intéressant  \n",
    "- 32k×32k et + : overlap souvent rentable, surtout sur cluster multi-nœuds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b520d22",
   "metadata": {},
   "source": [
    "## etape4_mpi_overlap\n",
    "**Description :** MPI + recouvrement : recouvrements des échanges d’halo non-bloquants avec le calcul Jacobi local. Réduit l'impact de la latence réseau.\n",
    "\n",
    "**Intérêt :** Cache la latence réseau en recouvrant communication et calcul local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8382b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f jacobi\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -Xcompiler \"-fopenmp\" -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.cu -o jacobi -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -lstdc++ \\\n",
      "\t-Xlinker --no-as-needed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape4_mpi_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be3815dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rang 1: ny_local=2047 offset=2048\n",
      "nx=4096 ny=4096 size=2\n",
      "Rang 0: ny_local=2047 offset=1\n",
      "Overlap: 1000 iters en 7.15792 s, norm=0.309412\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-bcfa.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Name             \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  -----------------------------\n",
      "     80.5        733237856       2004    365887.2    401344.0    100736    536160      41446.7  cudaMemcpy                   \n",
      "     11.0        100081184          2  50040592.0  50040592.0  43052736  57028448    9882320.7  cuIpcOpenMemHandle_v2        \n",
      "      3.1         28382528       2000     14191.3     13376.0     10464    573152      15836.1  cudaLaunchKernel             \n",
      "      1.6         14339200       2000      7169.6      7008.0      5504     37824       1874.4  cuMemcpyDtoDAsync_v2         \n",
      "      1.5         13545952         32    423311.0      2832.0      1696  12827424    2263803.8  cuStreamCreate               \n",
      "      1.4         12326848      26939       457.6       256.0       224     29600        533.8  cuEventQuery                 \n",
      "      0.5          4311712          4   1077928.0   1273632.0    240192   1524256     580776.2  cudaFree                     \n",
      "      0.3          2574720       2000      1287.4      1280.0       768     28032        846.4  cuEventRecord                \n",
      "      0.1           896160          4    224040.0    241008.0    165536    248608      39173.9  cudaMalloc                   \n",
      "      0.0           220352          2    110176.0    110176.0    103424    116928       9548.8  cuMemGetHandleForAddressRange\n",
      "      0.0           206240         32      6445.0      5968.0      4416     14336       2029.1  cuStreamDestroy_v2           \n",
      "      0.0           196160        828       236.9        96.0        32      3328        281.1  cuGetProcAddress_v2          \n",
      "      0.0            77376          4     19344.0     16800.0      4416     39360      17438.2  cudaMemset                   \n",
      "      0.0            55584        256       217.1       224.0        64      1792        159.9  cuEventDestroy_v2            \n",
      "      0.0            47968        256       187.4       128.0        96      5344        467.2  cuEventCreate                \n",
      "      0.0             5632          2      2816.0      2816.0      1600      4032       1719.7  cuInit                       \n",
      "      0.0             1248          2       624.0       624.0       224      1024        565.7  cuModuleGetLoadingMode       \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                      \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------------------\n",
      "    100.0         79176576       2000   39588.3   39488.0     39040     40576        297.1  jacobi_kernel(const float *, float *, int, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     99.0        695039933   2000  347520.0  359904.0    239904    393184      40582.6  [CUDA memcpy Device-to-Host]\n",
      "      0.9          6137312   2000    3068.7    3008.0      2304     80576       1754.7  [CUDA memcpy Peer-to-Peer]  \n",
      "      0.1           981600      4  245400.0  256112.0     85952    383424     159775.8  [CUDA memcpy Host-to-Device]\n",
      "      0.0            38272      4    9568.0    9520.0      8736     10496        946.8  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  67076.096   2000    33.538    33.538    33.538    33.538        0.000  [CUDA memcpy Device-to-Host]\n",
      "    134.283      4    33.571    33.571    33.571    33.571        0.000  [CUDA memcpy Host-to-Device]\n",
      "    134.283      4    33.571    33.571    33.571    33.571        0.000  [CUDA memset]               \n",
      "     32.768   2000     0.016     0.016     0.016     0.016        0.000  [CUDA memcpy Peer-to-Peer]  \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape4_mpi_overlap\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 2 ./jacobi 1000 4096 4096 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86582246",
   "metadata": {},
   "source": [
    "## etape5_nccl\n",
    "**Description :** NCCL : utilisation de la NVIDIA Collective Communications Library pour les échanges GPU à GPU. Montre les gains via NVLink ou PCIe haute bande passante.\n",
    "\n",
    "**Intérêt :** Exploite automatiquement le topologie NVLink/PCIe pour des échanges GPU efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86536f",
   "metadata": {},
   "source": [
    "### Introduction à NCCL\n",
    "\n",
    "NCCL (NVIDIA Collective Communications Library) permet des communications collectives efficaces entre plusieurs GPU, en exploitant la topologie matérielle (NVLink, PCIe).  \n",
    "Dans un contexte Jacobi multi-GPU, NCCL peut être utilisé pour échanger les halos entre GPUs sans repasser par le CPU.\n",
    "\n",
    "**Exemple minimal d'utilisation de NCCL pour un échange entre deux GPU :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d614a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o jacobi_mpi_nccl\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -dc kernel.cu -o kernel.o\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.o -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -o jacobi_mpi_nccl\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape5_nccl\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92d5c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCCL version 2.19.3+cuda12.3\n",
      "Single GPU jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998031\n",
      "  100, 0.448893\n",
      "  200, 0.267747\n",
      "  300, 0.197740\n",
      "  400, 0.159443\n",
      "  500, 0.134900\n",
      "  600, 0.117686\n",
      "  700, 0.104841\n",
      "  800, 0.094847\n",
      "  900, 0.086838\n",
      "Jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998049\n",
      "  100, 0.448911\n",
      "  200, 0.267771\n",
      "  300, 0.197757\n",
      "  400, 0.159458\n",
      "  500, 0.134914\n",
      "  600, 0.117695\n",
      "  700, 0.104854\n",
      "  800, 0.094863\n",
      "  900, 0.086849\n",
      "Num GPUs: 2.\n",
      "4096x4096: 1 GPU:  29.4556 s, 2 GPUs:  14.7631 s, speedup:     2.00, efficiency:    99.76 \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape5_nccl\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "#nsys profile -t cuda --stats=true --force-overwrite true -o main \n",
    "NCCL_DEBUG=WARN mpirun -np 2 ./jacobi_mpi_nccl -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1f7c0",
   "metadata": {},
   "source": [
    "## etape6_nccl_overlap\n",
    "**Description :** NCCL + recouvrement : superposition des collectifs NCCL avec le calcul sur GPU, cachant le coût de communication.\n",
    "\n",
    "**Intérêt :** Essentiel à forte densité GPU pour maintenir les cœurs occupés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e02e365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f *.o jacobi_mpi_nccl_overlap\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -dc kernel.cu -o kernel.o\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.o -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -o jacobi_mpi_nccl_overlap\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape6_nccl_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8971d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single GPU jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998031\n",
      "  100, 0.448893\n",
      "  200, 0.267747\n",
      "  300, 0.197740\n",
      "  400, 0.159443\n",
      "  500, 0.134900\n",
      "  600, 0.117686\n",
      "  700, 0.104841\n",
      "  800, 0.094847\n",
      "  900, 0.086838\n",
      "Jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998048\n",
      "  100, 0.448911\n",
      "  200, 0.267771\n",
      "  300, 0.197757\n",
      "  400, 0.159458\n",
      "  500, 0.134914\n",
      "  600, 0.117695\n",
      "  700, 0.104854\n",
      "  800, 0.094863\n",
      "  900, 0.086849\n",
      "Num GPUs: 2.\n",
      "4096x4096: 1 GPU:  29.4558 s, 2 GPUs:  15.2023 s, speedup:     1.94, efficiency:    96.88 \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape6_nccl_overlap\n",
    "#nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main \n",
    "mpirun -np 2 ./jacobi_mpi_nccl_overlap -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3c561",
   "metadata": {},
   "source": [
    "## etape7_nccl_graphs\n",
    "**Description :** NCCL + CUDA Graphs : capture et relecture des séquences Jacobi/échange pour réduire le surcoût des lancements.\n",
    "\n",
    "**Intérêt :** Réduit l’overhead de lancement grâce aux CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65978918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape7_nccl_graphs.csv ./main\n",
    "cat rapport_etape7_nccl_graphs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4aaa2",
   "metadata": {},
   "source": [
    "## etape8_nvshmem\n",
    "**Description :** NVSHMEM : modèle PGAS à accès mémoire unilatéral GPU, simplifiant les mises à jour d’halo.\n",
    "\n",
    "**Intérêt :** Simplifie les échanges via modèle PGAS unilatéral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a25a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape8_nvshmem.csv ./main\n",
    "cat rapport_etape8_nvshmem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9b0f5",
   "metadata": {},
   "source": [
    "## etape9_nvshmem_lt\n",
    "**Description :** NVSHMEM + LTO : ajout de l’optimisation link-time pour inliner les fonctions critiques et réduire le coût des appels.\n",
    "\n",
    "**Intérêt :** Optimisation link-time pour inliner les sections critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape9_nvshmem_lt\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape9_nvshmem_lt\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape9_nvshmem_lt.csv ./main\n",
    "cat rapport_etape9_nvshmem_lt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bbaef",
   "metadata": {},
   "source": [
    "## etape10_vshmem_neighborhood_lto\n",
    "**Description :** vshmem neighborhood_sync + LTO : synchronisation fine-grain de voisinage et optimisations link-time O2.\n",
    "\n",
    "**Intérêt :** Synchronisation fine et LTO pour boucles serrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape10_vshmem_neighborhood_lto.csv ./main \n",
    "cat rapport_etape10_vshmem_neighborhood_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0e44",
   "metadata": {},
   "source": [
    "## etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "**Description :** Combinaison : NVSHMEM avec recouvrement, synchrone de voisinage, et LTO pour maximiser la concurrence.\n",
    "\n",
    "**Intérêt :** Combinaison des meilleures pratiques pour un binaire ultra-optimisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv ./main\n",
    "cat rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33006f2a",
   "metadata": {},
   "source": [
    "## etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1\n",
    "**Description :** Tuning étendu : paramètres ajustables (taille de tuile, ordre de boucles) et hooks de benchmark.\n",
    "\n",
    "**Intérêt :** Ajout de paramètres de tuning et hooks de benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c846b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv ./main\n",
    "cat rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

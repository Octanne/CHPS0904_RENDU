{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17aa471",
   "metadata": {},
   "source": [
    "# Solveur de Jacobi : Modèles de programmation Multi-GPU\n",
    "Ce notebook présente 12 versions progressives d’un solveur de Jacobi 2D. Chaque section explique le modèle ou l’optimisation, compile la version, exécute et collecte les métriques Nsight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506cee8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Modules Spack à charger\n",
    "\n",
    "Avant de compiler ou d’exécuter les différentes étapes, il est recommandé de charger les modules nécessaires via Spack. \n",
    "Préférablement avant de lancer VSCode ou jupyter\n",
    "Par exemple :\n",
    "\n",
    "```bash\n",
    "spack load nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "```\n",
    "\n",
    "Adaptez la version de chaque module selon la configuration de votre cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8dae0a",
   "metadata": {},
   "source": [
    "## etape1_cpu\n",
    "**Description :** Solveur Jacobi CPU de base : implémentation mono-thread. Utile pour valider la correction et les petites tailles de problème ; met en évidence la limite de calcul CPU.\n",
    "\n",
    "**Intérêt :** Baseline : évalue la limite CPU pour établir une référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7495f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f main *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html\n",
      "gcc -O2 -o main main.c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape1_cpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75f86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape1_cpu\n",
      "CPU time: 10.315427 seconds\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape1_cpu\n",
    "./main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2f1db",
   "metadata": {},
   "source": [
    "## etape2_cpu_gpu\n",
    "**Description :** 1 CPU + 1 GPU + 1 stream : le CPU pilote le GPU via un unique stream CUDA. Le calcul Jacobi est entièrement délégué au GPU, le CPU ne fait que l’orchestration.\n",
    "\n",
    "**Intérêt :** Met en évidence l'écart de performance CPU vs GPU lorsque la grille est suffisamment grande, dans un contexte réaliste d’utilisation d’un seul GPU et d’un seul stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70bb1",
   "metadata": {},
   "source": [
    "### Compilation et exécution (1 CPU + 1 GPU + 1 stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61559afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html\n",
      "nvcc -O2 -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape2_cpu_gpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d30d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape2_cpu_gpu (1CPU + 1GPU + 1stream)\n",
      "GPU time: 1.540748 seconds\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-352f.qdstrm'\n",
      "[1/6] [========================100%] main.nsys-rep\n",
      "[2/6] [========================100%] main.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------\n",
      "     72.1        704315616          3  234771872.0  350497920.0    1334496  352483200  202165134.8  cudaMemcpyAsync       \n",
      "     16.2        158182816          1  158182816.0  158182816.0  158182816  158182816          0.0  cudaStreamCreate      \n",
      "     10.0         98101984       1001      98004.0      98112.0       1632     102400       3126.8  cudaStreamSynchronize \n",
      "      1.5         15092064       1000      15092.1       2720.0       1952   12158912     384406.2  cudaLaunchKernel      \n",
      "      0.1           606272          2     303136.0     303136.0     277408     328864      36384.9  cudaMalloc            \n",
      "      0.0           423776          2     211888.0     211888.0     155296     268480      80033.2  cudaFree              \n",
      "      0.0            14752          1      14752.0      14752.0      14752      14752          0.0  cudaStreamDestroy     \n",
      "      0.0             1760          1       1760.0       1760.0       1760       1760          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         92593154       1000   92593.2   92608.0     89632     93920        466.7  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ----------------------------\n",
      "     99.8        702915276      2  351457638.0  351457638.0  350476550  352438726    1387468.0  [CUDA memcpy Host-to-Device]\n",
      "      0.2          1309920      1    1309920.0    1309920.0    1309920    1309920          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    268.435      2   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Host-to-Device]\n",
      "    134.218      1   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape2_cpu_gpu\n",
    "nsys profile -t cuda --stats=true --force-overwrite true -o main ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20823ca",
   "metadata": {},
   "source": [
    "## etape3_mpi_gpus\n",
    "**Description :** MPI + GPUs : domaine réparti sur plusieurs rangs MPI, chacun pilotant un GPU. Illustrations des défis de mise à l'échelle multi-nœuds et des communications inter-rangs.\n",
    "\n",
    "**Intérêt :** Test de montée en charge inter-nœuds et coût MPI sur cluster multi-GPU.\n",
    "\n",
    "**Step :** \n",
    "- Initialiser MPI, déterminer le rang et le nombre de processus.\n",
    "- Associer chaque rang à un GPU différent.\n",
    "- Diviser la grille entre les rangs (découpage 1D vertical).\n",
    "- Gérer les échanges d’halos entre rangs voisins avec MPI_Sendrecv.\n",
    "- Synchroniser les échanges à chaque itération.\n",
    "- Nettoyer MPI à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0777a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html main.qdstrm\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -Xcompiler \"-fopenmp\" -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.cu -o app -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -lstdc++ \\\n",
      "\t-Xlinker --no-as-needed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape3_mpi_gpus\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Done: 1000 iters in 3.99646s, norm=1.72125\n",
      "Generating '/tmp/nsys-report-06a9.qdstrm'\n",
      "Generating '/tmp/nsys-report-37a5.qdstrm'\n",
      "Generating '/tmp/nsys-report-c030.qdstrm'\n",
      "Generating '/tmp/nsys-report-d175.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [========================100%] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/ProtobufComm/Common/ProtobufUtils.cpp(73): Throw in function void QuadDProtobufUtils::ReadMessage(QuadDProtobufUtils::PbCodedIStream&, QuadDProtobufUtils::PbMessageLite&)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::ProtobufParseException>\n",
      "std::exception::what: ProtobufParseException\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [0%                          ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: Section Table Reference magic number mismatch.\n",
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [======34%                   ] main.sqliteep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: LZ4 decompression failed.\n",
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------\n",
      "     95.8        249794528       5002   49938.9   12704.0      3808    365184      77647.1  cudaMemcpy            \n",
      "      3.9         10051040       1000   10051.0    9312.0      6656    538688      16959.9  cudaLaunchKernel      \n",
      "      0.1           336032          2  168016.0  168016.0     84800    251232     117685.2  cudaFree              \n",
      "      0.1           308928          2  154464.0  154464.0    118272    190656      51183.2  cudaMalloc            \n",
      "      0.0           109216        413     264.4      96.0        32      3712        338.0  cuGetProcAddress_v2   \n",
      "      0.0            51520          2   25760.0   25760.0      4640     46880      29868.2  cudaMemset            \n",
      "      0.0             4128          1    4128.0    4128.0      4128      4128          0.0  cuInit                \n",
      "      0.0              576          1     576.0     576.0       576       576          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                      \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------------------\n",
      "    100.0         15074496       1000   15074.5   15072.0     14816     15520        113.1  jacobi_kernel(const float *, float *, int, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     98.1        198268800   3000   66089.6    2400.0      1920    195648      90356.9  [CUDA memcpy Device-to-Host]\n",
      "      1.9          3762880   2002    1879.6    1568.0      1408    194048       6060.9  [CUDA memcpy Host-to-Device]\n",
      "      0.0            10176      2    5088.0    5088.0      4960      5216        181.0  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  16842.752   3000     5.614     0.016     0.016    16.810        7.918  [CUDA memcpy Device-to-Host]\n",
      "     66.388   2002     0.033     0.016     0.016    16.810        0.531  [CUDA memcpy Host-to-Device]\n",
      "     33.620      2    16.810    16.810    16.810    16.810        0.000  [CUDA memset]               \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape3_mpi_gpus\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "mpirun -np 4 nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main ./app 1000 4096 4096 1\n",
    "#mpirun -np 4 ./app 1000 4096 4096 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f4634",
   "metadata": {},
   "source": [
    "### À partir de quelle taille de matrice le recouvrement communication/calcul devient-il rentable ?\n",
    "\n",
    "Le recouvrement (overlap) communication/calcul devient généralement rentable lorsque :\n",
    "- Le temps de communication (MPI + transferts host/device) devient significatif devant le temps de calcul local.\n",
    "- La partie du calcul qui peut être effectuée pendant la communication (hors bords) est suffisamment grande pour masquer la latence réseau.\n",
    "\n",
    "Pour une grille Jacobi 2D, la taille critique dépend :\n",
    "- De la bande passante et latence réseau,\n",
    "- Du nombre de rangs MPI,\n",
    "- De la rapidité des transferts CUDA Host/Device,\n",
    "- De la puissance du GPU.\n",
    "\n",
    "**Sur la plupart des clusters modernes, le recouvrement commence à être rentable pour des matrices de l’ordre de 16k×16k à 32k×32k (voire plus),** surtout si le nombre de rangs est élevé (≥4) et que la communication devient un vrai goulot d’étranglement.\n",
    "\n",
    "**Pour une matrice 8k×8k,** le calcul local reste souvent dominant, donc le surcoût du overlap (copies, synchronisations) peut masquer le gain.  \n",
    "**Essayez avec 16k×16k ou 32k×32k** pour voir un bénéfice, surtout si vous augmentez le nombre de rangs MPI (et donc la proportion de communication).\n",
    "\n",
    "**Résumé :**  \n",
    "- < 8k×8k : overlap rarement utile  \n",
    "- 16k×16k : commence à être intéressant  \n",
    "- 32k×32k et + : overlap souvent rentable, surtout sur cluster multi-nœuds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b520d22",
   "metadata": {},
   "source": [
    "## etape4_mpi_overlap\n",
    "**Description :** MPI + recouvrement : recouvrements des échanges d’halo non-bloquants avec le calcul Jacobi local. Réduit l'impact de la latence réseau.\n",
    "\n",
    "**Intérêt :** Cache la latence réseau en recouvrant communication et calcul local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8382b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html main.qdstrm\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -O3 -std=c++14 -lcudart -Xcompiler \"-fopenmp\" -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include main.cpp kernel.cu -o app -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lmpi -lnccl -lstdc++ \\\n",
      "\t-Xlinker --no-as-needed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape4_mpi_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3815dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "nx=4096 ny=4096 size=4\n",
      "Rang 0: ny_local=1024 offset=1\n",
      "Rang 3: ny_local=1023 offset=3072\n",
      "Rang 1: ny_local=1024 offset=1025\n",
      "Rang 2: ny_local=1023 offset=2049\n",
      "Overlap: 1000 iters en 3.96817 s, norm=1.96709\n",
      "Generating '/tmp/nsys-report-c523.qdstrm'\n",
      "Generating '/tmp/nsys-report-d101.qdstrm'\n",
      "Generating '/tmp/nsys-report-a97c.qdstrm'\n",
      "Generating '/tmp/nsys-report-a50b.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [===================82%      ] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: Reading raw data failed, size: 60668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [0%                          ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/StreamSections/StreamWithSections.cpp(741): Throw in function void QuadDCommon::parseProtobufFromStream(std::istream&, google::protobuf::Message&)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::ProtobufParseException>\n",
      "std::exception::what: ProtobufParseException\n",
      "[boost::errinfo_api_function_*] = parseProtobufFromStream\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [0%                          ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/StreamSections/StreamWithSections.cpp(741): Throw in function void QuadDCommon::parseProtobufFromStream(std::istream&, google::protobuf::Message&)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::ProtobufParseException>\n",
      "std::exception::what: ProtobufParseException\n",
      "[boost::errinfo_api_function_*] = parseProtobufFromStream\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Name             \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -----------------------------\n",
      "     84.4        209431584       1002   209013.6   203840.0    201312   4571328     138135.4  cudaMemcpy                   \n",
      "      5.6         14017024       1000    14017.0    13632.0      9600    442912      13719.5  cudaLaunchKernel             \n",
      "      5.0         12387168       2000     6193.6     6768.0      2720     45536       2863.6  cuMemcpyAsync                \n",
      "      3.0          7325312      10926      670.4      288.0       224     89856       1115.7  cuEventQuery                 \n",
      "      1.1          2608736          1  2608736.0  2608736.0   2608736   2608736          0.0  cuMemHostRegister_v2         \n",
      "      0.6          1369696       2000      684.8      672.0       192      5728        385.1  cuEventRecord                \n",
      "      0.1           348224          2   174112.0   174112.0     67456    280768     150834.4  cudaFree                     \n",
      "      0.1           309504          2   154752.0   154752.0    125280    184224      41679.7  cudaMalloc                   \n",
      "      0.0           100288        414      242.2      192.0        32      3776        274.3  cuGetProcAddress_v2          \n",
      "      0.0            79456          1    79456.0    79456.0     79456     79456          0.0  cuMemGetHandleForAddressRange\n",
      "      0.0            48256          2    24128.0    24128.0      4544     43712      27696.0  cudaMemset                   \n",
      "      0.0            44192        128      345.3      320.0        96      2176        219.2  cuEventCreate                \n",
      "      0.0            38144          1    38144.0    38144.0     38144     38144          0.0  cuMemHostUnregister          \n",
      "      0.0            23424        128      183.0      128.0        96      1568        151.9  cuEventDestroy_v2            \n",
      "      0.0            14944          1    14944.0    14944.0     14944     14944          0.0  cuStreamDestroy_v2           \n",
      "      0.0            12288          1    12288.0    12288.0     12288     12288          0.0  cuStreamCreate               \n",
      "      0.0             4640          1     4640.0     4640.0      4640      4640          0.0  cuStreamSynchronize          \n",
      "      0.0             2496          1     2496.0     2496.0      2496      2496          0.0  cuInit                       \n",
      "      0.0              576          1      576.0      576.0       576       576          0.0  cuModuleGetLoadingMode       \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                      \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  -----------------------------------------------\n",
      "    100.0         15095327       1000   15095.3   15104.0     14816     15648        103.6  jacobi_kernel(const float *, float *, int, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     97.6        191690304   1000  191690.3  191616.0    191264    197216        359.8  [CUDA memcpy Device-to-Host]\n",
      "      2.4          4738848   2002    2367.1    1600.0      1408    194112       6081.2  [CUDA memcpy Host-to-Device]\n",
      "      0.0            10272      2    5136.0    5136.0      5024      5248        158.4  [CUDA memset]               \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "  16760.832   1000    16.761    16.761    16.761    16.761        0.000  [CUDA memcpy Device-to-Host]\n",
      "     66.355   2002     0.033     0.016     0.016    16.794        0.530  [CUDA memcpy Host-to-Device]\n",
      "     33.587      2    16.794    16.794    16.794    16.794        0.000  [CUDA memset]               \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape4_mpi_overlap\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "mpirun -np 4 nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main ./app 1000 4096 4096 1\n",
    "#mpirun -np 4 ./app 1000 4096 4096 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86582246",
   "metadata": {},
   "source": [
    "## etape5_nccl\n",
    "**Description :** NCCL : utilisation de la NVIDIA Collective Communications Library pour les échanges GPU à GPU. Montre les gains via NVLink ou PCIe haute bande passante.\n",
    "\n",
    "**Intérêt :** Exploite automatiquement le topologie NVLink/PCIe pour des échanges GPU efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86536f",
   "metadata": {},
   "source": [
    "### Introduction à NCCL\n",
    "\n",
    "NCCL (NVIDIA Collective Communications Library) permet des communications collectives efficaces entre plusieurs GPU, en exploitant la topologie matérielle (NVLink, PCIe).  \n",
    "Dans un contexte Jacobi multi-GPU, NCCL peut être utilisé pour échanger les halos entre GPUs sans repasser par le CPU.\n",
    "\n",
    "**Exemple minimal d'utilisation de NCCL pour un échange entre deux GPU :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d614a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html main.qdstrm\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -lineinfo -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -std=c++14 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include kernel.cu -c\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/bin/mpicxx -DUSE_NVTX -O3 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -std=c++14 main.cpp kernel.o -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lcudart -ldl -lnccl -o app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.cpp:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape5_nccl\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "NCCL version 2.19.3+cuda12.3\n",
      "Single GPU jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998032\n",
      "  100, 0.448893\n",
      "  200, 0.267747\n",
      "  300, 0.197740\n",
      "  400, 0.159443\n",
      "  500, 0.134900\n",
      "  600, 0.117686\n",
      "  700, 0.104841\n",
      "  800, 0.094847\n",
      "  900, 0.086838\n",
      "Jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998046\n",
      "  100, 0.448918\n",
      "  200, 0.267779\n",
      "  300, 0.197766\n",
      "  400, 0.159464\n",
      "  500, 0.134923\n",
      "  600, 0.117701\n",
      "  700, 0.104860\n",
      "  800, 0.094871\n",
      "  900, 0.086854\n",
      "Num GPUs: 4.\n",
      "4096x4096: 1 GPU:  29.8297 s, 4 GPUs:  14.6582 s, speedup:     2.04, efficiency:    50.88 \n",
      "Generating '/tmp/nsys-report-e8c0.qdstrm'\n",
      "Generating '/tmp/nsys-report-1b68.qdstrm'\n",
      "Generating '/tmp/nsys-report-70d0.qdstrm'\n",
      "Generating '/tmp/nsys-report-4806.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [======================91%   ] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: Reading raw data failed, size: 123451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [11%                         ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importer error status: Importation failed.\n",
      "Import Failed with unexpected exception: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/StreamSections/FileStream.cpp(368): Throw in function void QuadDCommon::FileStream::truncate(std::streamsize)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::InvalidArgumentException>\n",
      "std::exception::what: InvalidArgumentException\n",
      "[QuadDCommon::tag_message*] = Invalid truncate size.\n",
      "[QuadDCommon::tag_report_file_name*] = \"/gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.nsys-rep\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/7] [13%                         ] main.sqliteGenerated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.qdstrm\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Database file /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.sqlite could not be opened and appears to be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/7] [========================97% ] main.sqlite[4/7] Executing 'cuda_api_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Database file /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.sqlite could not be opened and appears to be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/7] [========================100%] main.sqlite[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                             Name                                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------------------------------------------------------------------------\n",
      "     89.3      36800081702       2000  18400040.9  18399664.0   7360064  29442240   11042168.5  void jacobi_kernel<(int)32, (int)32>(float *, const float *, float *, int, int, int, bool)\n",
      "     10.7       4428819801       1010   4384970.1   3831904.0      8416  27120544    4643524.2  ncclDevKernel_SendRecv(ncclDevComm *, unsigned long, ncclWork *)                          \n",
      "      0.0             5696          2      2848.0      2848.0      2656      3040        271.5  initialize_boundaries(float *, float *, float, int, int, int, int)                        \n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)   Style           Range        \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  -------  ---------------------\n",
      "     74.7       4925504768       5050      975347.5          96.0          32    57863712    2574100.6  PushPop  NCCL:ncclGroupEnd    \n",
      "     24.5       1617342848          1  1617342848.0  1617342848.0  1617342848  1617342848          0.0  PushPop  NCCL:ncclCommInitRank\n",
      "      0.6         41233376          1    41233376.0    41233376.0    41233376    41233376          0.0  PushPop  NCCL:ncclCommDestroy \n",
      "      0.1          3565216       2020        1765.0        1568.0         416       40800       1528.0  PushPop  NCCL:ncclRecv        \n",
      "      0.0          1464576       2020         725.0         608.0         416       18624        615.1  PushPop  NCCL:ncclSend        \n",
      "      0.0           928864       5050         183.9          96.0          32       33152        575.3  PushPop  NCCL:ncclGroupStart  \n",
      "\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)            Operation           \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "     40.6          3360928   2002    1678.8    1408.0      1376    396160       9098.4  [CUDA memcpy Device-to-Host]  \n",
      "     40.3          3333152   2000    1666.6    1632.0      1440      2112        102.1  [CUDA memcpy Device-to-Device]\n",
      "     18.8          1557088   2011     774.3     736.0       736     18944        583.4  [CUDA memset]                 \n",
      "      0.3            23584     28     842.3     768.0       736      1280        164.3  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                     Name                   \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------------------------\n",
      "     99.7      36823867168       2015  18274872.0  24772640.0      1632  49928864   12297456.8  cudaStreamSynchronize                     \n",
      "      0.1         21794944       4028      5410.9      5568.0      1312    212576       3720.3  cudaMemcpyAsync                           \n",
      "      0.1         18750848       2002      9366.1      9024.0      4736    414944      10867.4  cudaLaunchKernel                          \n",
      "      0.0         16181184       2007      8062.4      8064.0      2240     61312       2154.3  cudaMemsetAsync                           \n",
      "      0.0         13248256         26    509548.3    354368.0      4224   2900032     779805.9  cudaHostAlloc                             \n",
      "      0.0          5859936       1010      5801.9      4096.0      3008   1601696      50291.5  cudaLaunchKernelExC_v11060                \n",
      "      0.0          5178784       6029       859.0       672.0       320     24576        563.1  cudaStreamWaitEvent                       \n",
      "      0.0          5156128       6024       855.9       704.0       320     15744        462.2  cudaEventRecord                           \n",
      "      0.0          4179520         26    160750.8    110720.0      6592    884800     216839.8  cudaFreeHost                              \n",
      "      0.0          1203520          6    200586.7    166768.0     91808    417472     122338.1  cudaMalloc                                \n",
      "      0.0          1070784          8    133848.0    108304.0     18048    324928     106444.2  cudaFree                                  \n",
      "      0.0           577664       1010       571.9       576.0       224      3520        179.2  cudaStreamGetCaptureInfo_v2_v11030        \n",
      "      0.0           543296          2    271648.0    271648.0    126368    416928     205456.9  cudaMemcpy                                \n",
      "      0.0           452384          8     56548.0     40976.0     32448    155392      41511.6  cuMemSetAccess                            \n",
      "      0.0           410144          8     51268.0     38768.0     31936    105760      26273.6  cuMemUnmap                                \n",
      "      0.0           313696          8     39212.0     28672.0     23008     98240      25342.6  cuMemCreate                               \n",
      "      0.0           258016          2    129008.0    129008.0     27072    230944     144159.3  cudaStreamCreateWithFlags                 \n",
      "      0.0           174560        819       213.1       128.0        32      3584        257.4  cuGetProcAddress_v2                       \n",
      "      0.0           139648         16      8728.0      6928.0        32     26496       9507.0  cuMemRelease                              \n",
      "      0.0           127552          8     15944.0      9744.0      8416     44480      12690.9  cuMemMap                                  \n",
      "      0.0           103392          4     25848.0     14512.0      4480     69888      30578.9  cudaMemset                                \n",
      "      0.0            75488          6     12581.3      7984.0      4640     27392       9948.7  cudaStreamDestroy                         \n",
      "      0.0            70240        130       540.3       256.0        64      5792        815.1  cudaThreadExchangeStreamCaptureMode_v10010\n",
      "      0.0            67936          4     16984.0     10784.0      2880     43488      18888.4  cudaStreamCreate                          \n",
      "      0.0            41152          5      8230.4      7584.0      5472     12032       2602.0  cudaDeviceSynchronize                     \n",
      "      0.0            33312          8      4164.0      1488.0       768     15392       5492.2  cuMemAddressReserve                       \n",
      "      0.0            22976         29       792.3       672.0        64      2464        526.3  cuGetProcAddress                          \n",
      "      0.0            15008          6      2501.3      1008.0       288      6944       2941.7  cudaEventDestroy                          \n",
      "      0.0            14528          6      2421.3      2272.0       608      4608       1899.4  cudaEventCreateWithFlags                  \n",
      "      0.0            13536          8      1692.0      1232.0       736      4320       1205.1  cuMemAddressFree                          \n",
      "      0.0            12928          8      1616.0      1136.0       832      4576       1266.2  cuMemRetainAllocationHandle               \n",
      "      0.0            10240          8      1280.0       128.0        64      7264       2540.8  cuMemGetAllocationGranularity             \n",
      "      0.0             6560          3      2186.7      1792.0       800      3968       1620.5  cuInit                                    \n",
      "      0.0             1568          2       784.0       784.0       704       864        113.1  cuModuleGetLoadingMode                    \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)            Operation           \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "    167.858   2011     0.083     0.000     0.000    67.109        2.181  [CUDA memset]                 \n",
      "     83.894   2002     0.042     0.000     0.000    67.109        1.546  [CUDA memcpy Device-to-Host]  \n",
      "     32.768   2000     0.016     0.016     0.016     0.016        0.000  [CUDA memcpy Device-to-Device]\n",
      "      0.010     28     0.000     0.000     0.000     0.008        0.001  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.sqlite\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                             Name                                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------------------------------------------------------------------------\n",
      "     89.3      36800081702       2000  18400040.9  18399664.0   7360064  29442240   11042168.5  void jacobi_kernel<(int)32, (int)32>(float *, const float *, float *, int, int, int, bool)\n",
      "     10.7       4428819801       1010   4384970.1   3831904.0      8416  27120544    4643524.2  ncclDevKernel_SendRecv(ncclDevComm *, unsigned long, ncclWork *)                          \n",
      "      0.0             5696          2      2848.0      2848.0      2656      3040        271.5  initialize_boundaries(float *, float *, float, int, int, int, int)                        \n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)            Operation           \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "     40.6          3360928   2002    1678.8    1408.0      1376    396160       9098.4  [CUDA memcpy Device-to-Host]  \n",
      "     40.3          3333152   2000    1666.6    1632.0      1440      2112        102.1  [CUDA memcpy Device-to-Device]\n",
      "     18.8          1557088   2011     774.3     736.0       736     18944        583.4  [CUDA memset]                 \n",
      "      0.3            23584     28     842.3     768.0       736      1280        164.3  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)            Operation           \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "    167.858   2011     0.083     0.000     0.000    67.109        2.181  [CUDA memset]                 \n",
      "     83.894   2002     0.042     0.000     0.000    67.109        1.546  [CUDA memcpy Device-to-Host]  \n",
      "     32.768   2000     0.016     0.016     0.016     0.016        0.000  [CUDA memcpy Device-to-Device]\n",
      "      0.010     28     0.000     0.000     0.000     0.008        0.001  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape5_nccl/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape5_nccl\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "NCCL_DEBUG=WARN mpirun -np 4 nsys profile -t mpi,cuda,nvtx --stats=true --force-overwrite true -o main ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1\n",
    "#NCCL_DEBUG=WARN mpirun -np 4 ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1f7c0",
   "metadata": {},
   "source": [
    "## etape6_nccl_overlap\n",
    "**Description :** NCCL + recouvrement : superposition des collectifs NCCL avec le calcul sur GPU, cachant le coût de communication.\n",
    "\n",
    "**Intérêt :** Essentiel à forte densité GPU pour maintenir les cœurs occupés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e02e365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app *.o main.nsys-rep main.sqlite main.AnalysisSummary.html main.DiagnosticsSummary.html main.qdstrm\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -lineinfo -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -std=c++14 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include kernel.cu -c\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/bin/mpicxx -DUSE_NVTX -O3 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -std=c++14 main.cpp kernel.o -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lcudart -ldl -lnccl -o app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.cpp:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd etape6_nccl_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8971d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "NCCL version 2.19.3+cuda12.3\n",
      "Single GPU jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998031\n",
      "  100, 0.448893\n",
      "  200, 0.267746\n",
      "  300, 0.197740\n",
      "  400, 0.159443\n",
      "  500, 0.134900\n",
      "  600, 0.117686\n",
      "  700, 0.104841\n",
      "  800, 0.094847\n",
      "  900, 0.086838\n",
      "Jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998046\n",
      "  100, 0.448918\n",
      "  200, 0.267779\n",
      "  300, 0.197766\n",
      "  400, 0.159464\n",
      "  500, 0.134923\n",
      "  600, 0.117701\n",
      "  700, 0.104860\n",
      "  800, 0.094871\n",
      "  900, 0.086854\n",
      "Num GPUs: 4.\n",
      "4096x4096: 1 GPU:  29.8715 s, 4 GPUs:   8.9578 s, speedup:     3.33, efficiency:    83.37 \n",
      "Generating '/tmp/nsys-report-1e2b.qdstrm'\n",
      "Generating '/tmp/nsys-report-bf7c.qdstrm'\n",
      "Generating '/tmp/nsys-report-4357.qdstrm'\n",
      "Generating '/tmp/nsys-report-c351.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [==================77%       ] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/StreamSections/StreamWithSections.cpp(741): Throw in function void QuadDCommon::parseProtobufFromStream(std::istream&, google::protobuf::Message&)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::ProtobufParseException>\n",
      "std::exception::what: ProtobufParseException\n",
      "[boost::errinfo_api_function_*] = parseProtobufFromStream\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [===22%                      ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: LZ4 decompression failed.\n",
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [9%                          ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: LZ4 decompression failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/7] [10%                         ] main.sqlite[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/7] [===22%                      ] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export error: LZ4 decompression failed.\n",
      "FATAL ERROR: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Target/Daemon/Agent/OutputFile.cpp(726): Throw in function const boost::filesystem::path& QuadDDaemon::OutputFile::GetPath(QuadDDaemon::Extension) const\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::FileException>\n",
      "std::exception::what: FileException\n",
      "[QuadDCommon::tag_message*] = Output file was never created\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape6_nccl_overlap\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "NCCL_DEBUG=WARN mpirun -np 4 nsys profile -t mpi,cuda,nvtx --stats=true --force-overwrite true -o main ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1\n",
    "#NCCL_DEBUG=WARN mpirun -np 4 ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3c561",
   "metadata": {},
   "source": [
    "## etape7_nccl_graphs\n",
    "**Description :** NCCL + CUDA Graphs : capture et relecture des séquences Jacobi/échange pour réduire le surcoût des lancements.\n",
    "\n",
    "**Intérêt :** Réduit l’overhead de lancement grâce aux CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65978918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -lineinfo -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -std=c++14 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include kernel.cu -c\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/bin/mpicxx -DUSE_NVTX -O3 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -std=c++14 main.cpp kernel.o -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -lcudart -ldl -lnccl -o app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.cpp:\n",
      "\"main.cpp\", line 483: warning: variable \"a_new_reg_handle\" is used before its value is set [used_before_set]\n",
      "          NCCL_CALL(ncclCommDeregister(nccl_comm, a_new_reg_handle));\n",
      "          ^\n",
      "\n",
      "Remark: individual warnings can be suppressed with \"--diag_suppress <warning-name>\"\n",
      "\n",
      "\"main.cpp\", line 484: warning: variable \"a_reg_handle\" is used before its value is set [used_before_set]\n",
      "          NCCL_CALL(ncclCommDeregister(nccl_comm, a_reg_handle));\n",
      "          ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "122ec68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "Collecting data...\n",
      "NCCL version 2.19.3+cuda12.3\n",
      "Single GPU jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998031\n",
      "  100, 0.448893\n",
      "  200, 0.267747\n",
      "  300, 0.197740\n",
      "  400, 0.159443\n",
      "  500, 0.134900\n",
      "  600, 0.117686\n",
      "  700, 0.104841\n",
      "  800, 0.094847\n",
      "  900, 0.086838\n",
      "Jacobi relaxation: 1000 iterations on 4096 x 4096 mesh with norm check every 1 iterations\n",
      "    0, 15.998045\n",
      "  100, 0.448918\n",
      "  200, 0.267779\n",
      "  300, 0.197766\n",
      "  400, 0.159464\n",
      "  500, 0.134923\n",
      "  600, 0.117701\n",
      "  700, 0.104860\n",
      "  800, 0.094871\n",
      "  900, 0.086854\n",
      "Num GPUs: 4.\n",
      "4096x4096: 1 GPU:  29.7773 s, 4 GPUs:  20.2274 s, speedup:     1.47, efficiency:    36.80 \n",
      "Generating '/tmp/nsys-report-7f01.qdstrm'\n",
      "Generating '/tmp/nsys-report-f769.qdstrm'\n",
      "Generating '/tmp/nsys-report-87e8.qdstrm'\n",
      "Generating '/tmp/nsys-report-533d.qdstrm'\n",
      "[1/7] [=============60%            ] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importer error status: Importation failed.\n",
      "File is corrupted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.qdstrm\n",
      "[1/7] [==================76%       ] main.nsys-rep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importer error status: An unknown error occurred.\n",
      "Dynamic exception type: boost::filesystem::filesystem_error\n",
      "std::exception::what: boost::filesystem::file_size: No such file or directory [system:2]: \"/gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.nsys-rep\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [=====================89%    ] main.nsys-repGenerated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.qdstrm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importer error status: Importation failed.\n",
      "Import Failed with unexpected exception: /dvs/p4/build/sw/devtools/Agora/Rel/QuadD_Main/QuadD/Common/StreamSections/FileStream.cpp(368): Throw in function void QuadDCommon::FileStream::truncate(std::streamsize)\n",
      "Dynamic exception type: boost::wrapexcept<QuadDCommon::InvalidArgumentException>\n",
      "std::exception::what: InvalidArgumentException\n",
      "[QuadDCommon::tag_message*] = Invalid truncate size.\n",
      "[QuadDCommon::tag_report_file_name*] = \"/gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.nsys-rep\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [0%                          ] main.sqliteGenerated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.qdstrm\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)    Style              Range           \n",
      " --------  ---------------  ---------  -------------  -------------  -----------  -----------  ------------  -------  ---------------------------\n",
      "     96.3      50019851712          2  25009925856.0  25009925856.0  20227132192  29792719520  6763891665.7  PushPop  :Jacobi solve              \n",
      "      3.0       1574830784          1   1574830784.0   1574830784.0   1574830784   1574830784           0.0  PushPop  NCCL:ncclCommInitRank      \n",
      "      0.3        136148448          1    136148448.0    136148448.0    136148448    136148448           0.0  PushPop  :NCCL_Warmup               \n",
      "      0.2        114199968         70      1631428.1           96.0           32    103641504    12392920.4  PushPop  NCCL:ncclGroupEnd          \n",
      "      0.1         50431392          1     50431392.0     50431392.0     50431392     50431392           0.0  PushPop  :Graph warmup              \n",
      "      0.1         42671136          1     42671136.0     42671136.0     42671136     42671136           0.0  PushPop  NCCL:ncclCommDestroy       \n",
      "      0.0         11333152       1004        11288.0         2880.0         1376      8024800      253158.9  PushPop  NCCL:hostStreamPlanCallback\n",
      "      0.0          1164224          1      1164224.0      1164224.0      1164224      1164224           0.0  PushPop  :Build graphs              \n",
      "      0.0            81696         28         2917.7         1280.0          448        39104        7246.8  PushPop  NCCL:ncclRecv              \n",
      "      0.0            19936         28          712.0          608.0          416         2176         372.3  PushPop  NCCL:ncclSend              \n",
      "      0.0            13312         70          190.2           96.0           32         1536         259.3  PushPop  NCCL:ncclGroupStart        \n",
      "\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                     Name                   \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------------------------\n",
      "     84.3      39551637120       2021  19570330.1  26961280.0      1632  49826688   10612899.2  cudaStreamSynchronize                     \n",
      "     15.5       7278987680       1004   7249987.7   8072224.0     13184  26015744    2297485.2  cudaGraphLaunch_v10000                    \n",
      "      0.0         15564320       3034      5130.0      5344.0      1344    238176       4624.6  cudaMemcpyAsync                           \n",
      "      0.0         12820032         26    493078.2    406576.0      3744   2883424     773052.3  cudaHostAlloc                             \n",
      "      0.0          9578304       1014      9446.1      9184.0       544    257376       7964.5  cudaLaunchKernel                          \n",
      "      0.0          8080864       1011      7992.9      7904.0       640     56736       2121.2  cudaMemsetAsync                           \n",
      "      0.0          3908768         26    150337.2    104224.0      7232    819232     198502.0  cudaFreeHost                              \n",
      "      0.0          3054208       4040       756.0       512.0       288     11200        499.4  cudaStreamWaitEvent                       \n",
      "      0.0          2208128       3037       727.1       576.0       320     11200        414.5  cudaEventRecord                           \n",
      "      0.0          1687840         14    120560.0      4080.0      1376   1609088     428459.8  cudaLaunchKernelExC_v11060                \n",
      "      0.0          1268128          6    211354.7    181280.0     95744    433536     120519.3  cudaMalloc                                \n",
      "      0.0          1148864          8    143608.0    107648.0     18432    341504     124660.8  cudaFree                                  \n",
      "      0.0           645120         12     53760.0     41792.0     30464    141696      31587.1  cuMemSetAccess                            \n",
      "      0.0           582528         12     48544.0     39936.0     31648     84224      20021.2  cuMemUnmap                                \n",
      "      0.0           557664          2    278832.0    278832.0    136480    421184     201316.1  cudaMemcpy                                \n",
      "      0.0           539360         12     44946.7     35520.0     20320    109792      26746.8  cuMemCreate                               \n",
      "      0.0           299168          6     49861.3     10032.0      3840    240512      93986.5  cudaStreamCreateWithFlags                 \n",
      "      0.0           298240          4     74560.0     61216.0     53440    122368      32644.9  cudaGraphExecDestroy_v10000               \n",
      "      0.0           295360          4     73840.0     69280.0     22336    134464      58347.8  cudaGraphInstantiateWithFlags_v11040      \n",
      "      0.0           217568         24      9065.3      6464.0        32     26912       9620.6  cuMemRelease                              \n",
      "      0.0           192128         12     16010.7     13024.0      8800     38784       9204.9  cuMemMap                                  \n",
      "      0.0           159808        819       195.1        96.0        32      4640        257.9  cuGetProcAddress_v2                       \n",
      "      0.0           122976          4     30744.0     20000.0      4704     78272      34775.5  cudaMemset                                \n",
      "      0.0            85952         11      7813.8      5056.0      3904     25600       6256.6  cudaStreamDestroy                         \n",
      "      0.0            79680        162       491.9       208.0        64      6112        810.1  cudaThreadExchangeStreamCaptureMode_v10010\n",
      "      0.0            54368          4     13592.0      7488.0      6272     33120      13033.4  cudaGraphUpload_v10000                    \n",
      "      0.0            42144         12      3512.0      1440.0       800     12608       4294.5  cuMemAddressReserve                       \n",
      "      0.0            35232          4      8808.0      8368.0      4480     14016       3929.7  cudaDeviceSynchronize                     \n",
      "      0.0            27680          3      9226.7      3616.0      2752     21312      10475.1  cudaStreamCreate                          \n",
      "      0.0            20320         29       700.7       608.0        96      2208        452.3  cuGetProcAddress                          \n",
      "      0.0            20128          2     10064.0     10064.0      2656     17472      10476.5  cudaStreamCreateWithPriority              \n",
      "      0.0            19424         12      1618.7      1168.0       736      4000        980.3  cuMemAddressFree                          \n",
      "      0.0            19232          8      2404.0       800.0       352      8864       3071.1  cudaEventDestroy                          \n",
      "      0.0            19136          4      4784.0      3296.0      2496     10048       3535.7  cudaGraphDestroy_v10000                   \n",
      "      0.0            18816          4      4704.0      2032.0      1472     13280       5739.6  cudaStreamBeginCapture_v10000             \n",
      "      0.0            18048         12      1504.0       928.0       576      6112       1547.1  cuMemRetainAllocationHandle               \n",
      "      0.0            15296          8      1912.0       688.0       256      4512       2023.9  cudaEventCreateWithFlags                  \n",
      "      0.0            11648         18       647.1       320.0       128      2880        804.8  cudaStreamGetCaptureInfo_v2_v11030        \n",
      "      0.0            11520         12       960.0       112.0        64      6496       1898.1  cuMemGetAllocationGranularity             \n",
      "      0.0             6656         12       554.7       176.0        96      4128       1141.7  cudaUserObjectCreate_v11030               \n",
      "      0.0             6336          4      1584.0      1072.0       832      3360       1189.5  cudaStreamEndCapture_v10000               \n",
      "      0.0             6112          8       764.0       336.0       128      3872       1263.7  cudaGraphAddEventWaitNode_v11010          \n",
      "      0.0             5696          8       712.0       464.0       224      2176        631.9  cudaGraphAddEventRecordNode_v11010        \n",
      "      0.0             5664          4      1416.0       576.0       352      4160       1834.1  cudaGraphAddHostNode_v10000               \n",
      "      0.0             5632          3      1877.3      1664.0       544      3424       1451.8  cuInit                                    \n",
      "      0.0             5056         12       421.3       240.0       128      2304        599.7  cudaGraphRetainUserObject_v11030          \n",
      "      0.0             3488          8       436.0       288.0       128      1184        371.2  cudaStreamUpdateCaptureDependencies_v11030\n",
      "      0.0             1600          2       800.0       800.0       704       896        135.8  cuModuleGetLoadingMode                    \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                             Name                                           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------------------------------------------------------------------------\n",
      "     99.9      29487039018       1000  29487039.0  29486800.5  29473504  29503775       5550.2  void jacobi_kernel<(int)32, (int)32>(float *, const float *, float *, int, int, int, bool)\n",
      "      0.1         30821215         10   3082121.5    573232.0     28928  11689727    4217661.5  ncclDevKernel_SendRecv(ncclDevComm *, unsigned long, ncclWork *)                          \n",
      "      0.0             5856          2      2928.0      2928.0      2848      3008        113.1  initialize_boundaries(float *, float *, float, int, int, int, int)                        \n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)            Operation           \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "     53.7          3337376   2000    1668.7    1664.0      1472      2080        104.0  [CUDA memcpy Device-to-Device]\n",
      "     32.0          1990272   1002    1986.3    1472.0      1440    396832      12876.8  [CUDA memcpy Device-to-Host]  \n",
      "     13.8           859040   1011     849.7     800.0       768     18976        815.0  [CUDA memset]                 \n",
      "      0.5            29056     32     908.0     800.0       736      1344        194.5  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)            Operation           \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "    167.822   1011     0.166     0.000     0.000    67.109        3.074  [CUDA memset]                 \n",
      "     83.874   1002     0.084     0.000     0.000    67.109        2.185  [CUDA memcpy Device-to-Host]  \n",
      "     32.768   2000     0.016     0.016     0.016     0.016        0.000  [CUDA memcpy Device-to-Device]\n",
      "      0.012     32     0.000     0.000     0.000     0.008        0.001  [CUDA memcpy Host-to-Device]  \n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape7_nccl_graphs/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "NCCL_DEBUG=WARN mpirun -np 4 nsys profile -t mpi,cuda,nvtx --stats=true --force-overwrite true -o main ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1\n",
    "#NCCL_DEBUG=WARN mpirun -np 4 ./app -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4aaa2",
   "metadata": {},
   "source": [
    "## etape8_nvshmem\n",
    "**Description :** NVSHMEM : modèle PGAS à accès mémoire unilatéral GPU, simplifiant les mises à jour d’halo.\n",
    "\n",
    "**Intérêt :** Simplifie les échanges via modèle PGAS unilatéral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44a25a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -Xptxas --optimize-float-atomics -ccbin=mpic++ -dc -Xcompiler -fopenmp -lineinfo -DUSE_NVTX -ldl -gencode arch=compute_90,code=sm_90 -std=c++14 -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nvshmem/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nvshmem/include/nvshmem main.cu -c -o main.o\n",
      "/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/bin/nvcc -gencode arch=compute_90,code=sm_90 main.o -o main -ccbin=mpic++ -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nvshmem/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nvshmem/lib/nvshmem -lcuda -lcudart -ldl -lnvidia-ml -lnvshmem\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "# Nsys ne marche pas tout le temps avec mpirun\n",
    "NCCL_DEBUG=WARN mpirun -np 4 nsys profile -t mpi,cuda,nvtx --stats=true --force-overwrite true -o main ./main -niter 1000 -nx 4096 -ny 4096 -nccheck 1\n",
    "#NCCL_DEBUG=WARN mpirun -np 4 ./main -niter 1000 -nx 4096 -ny 4096 -nccheck 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519399a",
   "metadata": {},
   "source": [
    "**Remarque :**\n",
    "\n",
    "Les versions utilisant LTO (Link-Time Optimization) ne sont pas disponibles ici. En effet, pour que LTO fonctionne avec NVSHMEM, il est impératif que NVSHMEM lui-même soit compilé avec l’option LTO activée. Or, dans notre environnement, NVSHMEM n’a pas été compilé avec cette option, ce qui rend impossible la génération des exécutables LTO pour ces étapes.\n",
    "Résultat : les étapes nécessitant LTO ne peuvent pas être testées dans ce notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9b0f5",
   "metadata": {},
   "source": [
    "## etape9_nvshmem_lto\n",
    "**Description :** NVSHMEM + LTO : ajout de l’optimisation link-time pour inliner les fonctions critiques et réduire le coût des appels.\n",
    "\n",
    "**Intérêt :** Optimisation link-time pour inliner les sections critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape9_nvshmem_lt\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape9_nvshmem_lt\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape9_nvshmem_lt.csv ./main\n",
    "cat rapport_etape9_nvshmem_lt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bbaef",
   "metadata": {},
   "source": [
    "## etape10_vshmem_neighborhood_lto\n",
    "**Description :** vshmem neighborhood_sync + LTO : synchronisation fine-grain de voisinage et optimisations link-time O2.\n",
    "\n",
    "**Intérêt :** Synchronisation fine et LTO pour boucles serrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape10_vshmem_neighborhood_lto.csv ./main \n",
    "cat rapport_etape10_vshmem_neighborhood_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0e44",
   "metadata": {},
   "source": [
    "## etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "**Description :** Combinaison : NVSHMEM avec recouvrement, synchrone de voisinage, et LTO pour maximiser la concurrence.\n",
    "\n",
    "**Intérêt :** Combinaison des meilleures pratiques pour un binaire ultra-optimisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv ./main\n",
    "cat rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33006f2a",
   "metadata": {},
   "source": [
    "## etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1\n",
    "**Description :** Tuning étendu : paramètres ajustables (taille de tuile, ordre de boucles) et hooks de benchmark.\n",
    "\n",
    "**Intérêt :** Ajout de paramètres de tuning et hooks de benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c846b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv ./main\n",
    "cat rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

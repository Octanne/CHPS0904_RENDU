{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17aa471",
   "metadata": {},
   "source": [
    "# Solveur de Jacobi : Modèles de programmation Multi-GPU\n",
    "Ce notebook présente 12 versions progressives d’un solveur de Jacobi 2D. Chaque section explique le modèle ou l’optimisation, compile la version, exécute et collecte les métriques Nsight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506cee8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Modules Spack à charger\n",
    "\n",
    "Avant de compiler ou d’exécuter les différentes étapes, il est recommandé de charger les modules nécessaires via Spack. Par exemple :\n",
    "\n",
    "```bash\n",
    "spack load cuda@12.6\n",
    "spack load nvidia-nsight-systems@2024.6.1\n",
    "spack load nvhpc@24.11 \n",
    "spack load cudnn@9.2.0.82-12\n",
    "```\n",
    "\n",
    "Adaptez la version de chaque module selon la configuration de votre cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8dae0a",
   "metadata": {},
   "source": [
    "## etape1_cpu\n",
    "**Description :** Solveur Jacobi CPU de base : implémentation mono-thread. Utile pour valider la correction et les petites tailles de problème ; met en évidence la limite de calcul CPU.\n",
    "\n",
    "**Intérêt :** Baseline : évalue la limite CPU pour établir une référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7495f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f main\n",
      "gcc -O2 -o main main.c\n",
      "gcc -O2 -o main main.c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75f86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape1_cpu\n",
      "CPU time: 12.633750 seconds\n",
      "CPU time: 12.633750 seconds\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-ea89.qdstrm'\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-ea89.qdstrm'\n",
      "[1/8] [========================100%] main.nsys-rep\n",
      "[1/8] [========================100%] main.nsys-rep\n",
      "[2/8] [========================100%] main.sqlite\n",
      "[2/8] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "SKIPPED: No data available.\n",
      "SKIPPED: No data available.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA trace data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA kernel data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain CUDA kernel data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite does not contain GPU memory data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape1_cpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape1_cpu\n",
    "nsys profile --stats=true --force-overwrite true -o main ./main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2f1db",
   "metadata": {},
   "source": [
    "## etape2_cpu_gpu\n",
    "**Description :** 1 CPU + 1 GPU + 1 stream : le CPU pilote le GPU via un unique stream CUDA. Le calcul Jacobi est entièrement délégué au GPU, le CPU ne fait que l’orchestration.\n",
    "\n",
    "**Intérêt :** Met en évidence l'écart de performance CPU vs GPU lorsque la grille est suffisamment grande, dans un contexte réaliste d’utilisation d’un seul GPU et d’un seul stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70bb1",
   "metadata": {},
   "source": [
    "### Compilation et exécution (1 CPU + 1 GPU + 1 stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61559afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -o app main.cu kernel.cu\n",
      "nvcc -O2 -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d30d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé etape2_cpu_gpu (1CPU + 1GPU + 1stream)\n",
      "GPU time: 1.518556 seconds\n",
      "GPU time: 1.518556 seconds\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-dd64.qdstrm'\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-dd64.qdstrm'\n",
      "[1/6] [========================100%] main.nsys-rep\n",
      "[1/6] [========================100%] main.nsys-rep\n",
      "[2/6] [========================100%] main.sqlite\n",
      "[2/6] [========================100%] main.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------\n",
      "     73.4        706122688          3  235374229.3  351699424.0    1335456  353087808  202684712.0  cudaMemcpyAsync       \n",
      "     15.7        150938016          1  150938016.0  150938016.0  150938016  150938016          0.0  cudaStreamCreate      \n",
      "     10.4         99869248       1001      99769.5      98304.0       1728    1522912      45280.2  cudaStreamSynchronize \n",
      "      0.4          3534880       1000       3534.9       2976.0       2016     461696      14522.2  cudaLaunchKernel      \n",
      "      0.1           666752          2     333376.0     333376.0     296704     370048      51862.0  cudaMalloc            \n",
      "      0.0           420896          2     210448.0     210448.0     154368     266528      79309.1  cudaFree              \n",
      "      0.0            13184          1      13184.0      13184.0      13184      13184          0.0  cudaStreamDestroy     \n",
      "      0.0             2048          1       2048.0       2048.0       2048       2048          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         92668733       1000   92668.7   92672.0     89440     94080        473.3  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ----------------------------\n",
      "     99.8        704714407      2  352357203.5  352357203.5  351669684  353044723     972299.4  [CUDA memcpy Host-to-Device]\n",
      "      0.2          1311648      1    1311648.0    1311648.0    1311648    1311648          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    268.435      2   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Host-to-Device]\n",
      "    134.218      1   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------\n",
      "     73.4        706122688          3  235374229.3  351699424.0    1335456  353087808  202684712.0  cudaMemcpyAsync       \n",
      "     15.7        150938016          1  150938016.0  150938016.0  150938016  150938016          0.0  cudaStreamCreate      \n",
      "     10.4         99869248       1001      99769.5      98304.0       1728    1522912      45280.2  cudaStreamSynchronize \n",
      "      0.4          3534880       1000       3534.9       2976.0       2016     461696      14522.2  cudaLaunchKernel      \n",
      "      0.1           666752          2     333376.0     333376.0     296704     370048      51862.0  cudaMalloc            \n",
      "      0.0           420896          2     210448.0     210448.0     154368     266528      79309.1  cudaFree              \n",
      "      0.0            13184          1      13184.0      13184.0      13184      13184          0.0  cudaStreamDestroy     \n",
      "      0.0             2048          1       2048.0       2048.0       2048       2048          0.0  cuModuleGetLoadingMode\n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         92668733       1000   92668.7   92672.0     89440     94080        473.3  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ----------------------------\n",
      "     99.8        704714407      2  352357203.5  352357203.5  351669684  353044723     972299.4  [CUDA memcpy Host-to-Device]\n",
      "      0.2          1311648      1    1311648.0    1311648.0    1311648    1311648          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    268.435      2   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Host-to-Device]\n",
      "    134.218      1   134.218   134.218   134.218   134.218        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape2_cpu_gpu/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6\n",
    "cd etape2_cpu_gpu\n",
    "nsys profile -t cuda --stats=true --force-overwrite true -o main ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20823ca",
   "metadata": {},
   "source": [
    "## etape3_mpi_gpus\n",
    "**Description :** MPI + GPUs : domaine réparti sur plusieurs rangs MPI, chacun pilotant un GPU. Illustrations des défis de mise à l'échelle multi-nœuds et des communications inter-rangs.\n",
    "\n",
    "**Intérêt :** Test de montée en charge inter-nœuds et coût MPI sur cluster multi-GPU.\n",
    "\n",
    "**Step :** \n",
    "- Initialiser MPI, déterminer le rang et le nombre de processus.\n",
    "- Associer chaque rang à un GPU différent.\n",
    "- Diviser la grille entre les rangs (découpage 1D vertical).\n",
    "- Gérer les échanges d’halos entre rangs voisins avec MPI_Sendrecv.\n",
    "- Synchroniser les échanges à chaque itération.\n",
    "- Nettoyer MPI à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0777a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -lcudart -lmpi -lnccl -lstdc++ -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n",
      "nvcc -O2 -lcudart -lmpi -lnccl -lstdc++ -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape3_mpi_gpus\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'étape 3 : MPI + multi-GPU\n",
      "Grille de taille 4096 x 4096, T = 1000 itérations\n",
      "Grille de taille 4096 x 4096, T = 1000 itérations\n",
      "Terminé etape3_mpi_gpus (MPI + multi-GPU) rank 0/4\n",
      "Max GPU time: 0.573540 seconds rank 0/4\n",
      "Terminé etape3_mpi_gpus (MPI + multi-GPU) rank 0/4\n",
      "Max GPU time: 0.573540 seconds rank 0/4\n",
      "Generating '/tmp/nsys-report-1c77.qdstrm'\n",
      "Generating '/tmp/nsys-report-1c77.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  --------  --------  --------  -----------  ----------------------\n",
      "     84.1        787842048      12012    65587.9    8224.0      3168  88112512    2256311.0  cudaMemcpy            \n",
      "     12.9        120601856       4000    30150.5   29504.0      3680   2358400      36872.6  cudaDeviceSynchronize \n",
      "      1.6         15403584       4000     3850.9    3104.0      2080    518208      16143.0  cudaLaunchKernel      \n",
      "      0.9          8369792          8  1046224.0  567360.0    486528   2878880     912372.8  cudaMalloc            \n",
      "      0.4          3630144          8   453768.0  519568.0     84416    679008     208549.3  cudaFree              \n",
      "      0.0           435360       1652      263.5     128.0        32      5216        312.2  cuGetProcAddress_v2   \n",
      "      0.0            10944          4     2736.0    2800.0      1952      3392        632.8  cuInit                \n",
      "      0.0             2368          4      592.0     592.0       448       736        118.3  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         98008385       4000   24502.1   24480.0     21568     25632        321.2  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     97.8        714380580   6008  118904.9    2432.0      2208  88079679    3188421.6  [CUDA memcpy Host-to-Device]\n",
      "      2.2         16250016   6004    2706.5    1856.0      1664   2855584      40850.4  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    464.912   6008     0.077     0.033     0.033    33.554        1.222  [CUDA memcpy Host-to-Device]\n",
      "    330.760   6004     0.055     0.033     0.033    33.554        0.865  [CUDA memcpy Device-to-Host]\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  --------  --------  --------  -----------  ----------------------\n",
      "     84.1        787842048      12012    65587.9    8224.0      3168  88112512    2256311.0  cudaMemcpy            \n",
      "     12.9        120601856       4000    30150.5   29504.0      3680   2358400      36872.6  cudaDeviceSynchronize \n",
      "      1.6         15403584       4000     3850.9    3104.0      2080    518208      16143.0  cudaLaunchKernel      \n",
      "      0.9          8369792          8  1046224.0  567360.0    486528   2878880     912372.8  cudaMalloc            \n",
      "      0.4          3630144          8   453768.0  519568.0     84416    679008     208549.3  cudaFree              \n",
      "      0.0           435360       1652      263.5     128.0        32      5216        312.2  cuGetProcAddress_v2   \n",
      "      0.0            10944          4     2736.0    2800.0      1952      3392        632.8  cuInit                \n",
      "      0.0             2368          4      592.0     592.0       448       736        118.3  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                   Name                 \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------------------\n",
      "    100.0         98008385       4000   24502.1   24480.0     21568     25632        321.2  jacobi_kernel(double *, double *, int)\n",
      "\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     97.8        714380580   6008  118904.9    2432.0      2208  88079679    3188421.6  [CUDA memcpy Host-to-Device]\n",
      "      2.2         16250016   6004    2706.5    1856.0      1664   2855584      40850.4  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "    464.912   6008     0.077     0.033     0.033    33.554        1.222  [CUDA memcpy Host-to-Device]\n",
      "    330.760   6004     0.055     0.033     0.033    33.554        0.865  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.sqlite\n",
      "\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape3_mpi_gpus/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "cd etape3_mpi_gpus\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 4 ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b520d22",
   "metadata": {},
   "source": [
    "## etape4_mpi_overlap\n",
    "**Description :** MPI + recouvrement : recouvrements des échanges d’halo non-bloquants avec le calcul Jacobi local. Réduit l'impact de la latence réseau.\n",
    "\n",
    "**Intérêt :** Cache la latence réseau en recouvrant communication et calcul local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8382b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f app\n",
      "nvcc -O2 -lcudart -lmpi -lnccl -lstdc++ -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/include -I/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/include -Xlinker --no-as-needed -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/cuda/12.6/lib64 -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib -L/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib -o app main.cu kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "cd etape4_mpi_overlap\n",
    "make clean all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be3815dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de l'étape 3 : MPI + multi-GPU\n",
      "Grille de taille 4096 x 4096, T = 1000 itérations\n",
      "Terminé etape3_mpi_gpus (MPI + multi-GPU) rank 0/4\n",
      "Max GPU time: 0.794325 seconds rank 0/4\n",
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-e8bb.qdstrm'\n",
      "[1/7] [========================100%] main.nsys-rep\n",
      "[2/7] [========================100%] main.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SKIPPED: No data available.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite does not contain CUDA kernel data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite does not contain GPU memory data.\n",
      "SKIPPED: /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite does not contain GPU memory data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  --------  --------  --------  ---------  -----------  ----------------------\n",
      "     63.0       1214923168       8000  151865.4     256.0       160  322465440    6794687.2  cudaDeviceSynchronize \n",
      "     36.5        704306144      12012   58633.5     288.0       160   88106560    2257784.4  cudaMemcpy            \n",
      "      0.3          5333856      10000     533.4     320.0       192     594144      10022.3  cudaLaunchKernel      \n",
      "      0.1          2684928          8  335616.0  306272.0    146464     598368     148155.5  cudaMalloc            \n",
      "      0.0           367776       1652     222.6      96.0        32       4704        278.2  cuGetProcAddress_v2   \n",
      "      0.0            89824          8   11228.0   10656.0      2432      18592       7148.0  cudaFree              \n",
      "      0.0            12320          4    3080.0    3392.0      2048       3488        693.9  cuInit                \n",
      "      0.0             2144          4     536.0     560.0       448        576         60.6  cuModuleGetLoadingMode\n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "Generated:\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.nsys-rep\n",
      "    /gpfs/home/colevalet/Cours/CHPS0904_RENDU/MultiGPU Programming Model/etape4_mpi_overlap/main.sqlite\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd etape4_mpi_overlap\n",
    "spack load nvidia-nsight-systems@2024.6.1 cuda@12.6 nvhpc@24.11\n",
    "export LD_LIBRARY_PATH=/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/12.6/hpcx/hpcx-2.20/ompi/lib:/apps/2025/manual_install/nvhpc/24.11/Linux_aarch64/24.11/comm_libs/nccl/lib:$LD_LIBRARY_PATH\n",
    "unset OPAL_PREFIX\n",
    "unset PMIX_INSTALL_PREFIX\n",
    "nsys profile -t mpi,cuda --stats=true --force-overwrite true -o main mpirun -np 4 ./app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86582246",
   "metadata": {},
   "source": [
    "## etape5_nccl\n",
    "**Description :** NCCL : utilisation de la NVIDIA Collective Communications Library pour les échanges GPU à GPU. Montre les gains via NVLink ou PCIe haute bande passante.\n",
    "\n",
    "**Intérêt :** Exploite automatiquement le topologie NVLink/PCIe pour des échanges GPU efficaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d614a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd etape5_nccl\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape5_nccl\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape5_nccl.csv ./main\n",
    "cat rapport_etape5_nccl.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1f7c0",
   "metadata": {},
   "source": [
    "## etape6_nccl_overlap\n",
    "**Description :** NCCL + recouvrement : superposition des collectifs NCCL avec le calcul sur GPU, cachant le coût de communication.\n",
    "\n",
    "**Intérêt :** Essentiel à forte densité GPU pour maintenir les cœurs occupés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape6_nccl_overlap\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape6_nccl_overlap \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape6_nccl_overlap.csv ./main\n",
    "cat rapport_etape6_nccl_overlap.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3c561",
   "metadata": {},
   "source": [
    "## etape7_nccl_graphs\n",
    "**Description :** NCCL + CUDA Graphs : capture et relecture des séquences Jacobi/échange pour réduire le surcoût des lancements.\n",
    "\n",
    "**Intérêt :** Réduit l’overhead de lancement grâce aux CUDA Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65978918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape7_nccl_graphs\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape7_nccl_graphs.csv ./main\n",
    "cat rapport_etape7_nccl_graphs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4aaa2",
   "metadata": {},
   "source": [
    "## etape8_nvshmem\n",
    "**Description :** NVSHMEM : modèle PGAS à accès mémoire unilatéral GPU, simplifiant les mises à jour d’halo.\n",
    "\n",
    "**Intérêt :** Simplifie les échanges via modèle PGAS unilatéral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a25a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape8_nvshmem\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape8_nvshmem.csv ./main\n",
    "cat rapport_etape8_nvshmem.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9b0f5",
   "metadata": {},
   "source": [
    "## etape9_nvshmem_lt\n",
    "**Description :** NVSHMEM + LTO : ajout de l’optimisation link-time pour inliner les fonctions critiques et réduire le coût des appels.\n",
    "\n",
    "**Intérêt :** Optimisation link-time pour inliner les sections critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape9_nvshmem_lt\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape9_nvshmem_lt\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape9_nvshmem_lt.csv ./main\n",
    "cat rapport_etape9_nvshmem_lt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bbaef",
   "metadata": {},
   "source": [
    "## etape10_vshmem_neighborhood_lto\n",
    "**Description :** vshmem neighborhood_sync + LTO : synchronisation fine-grain de voisinage et optimisations link-time O2.\n",
    "\n",
    "**Intérêt :** Synchronisation fine et LTO pour boucles serrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape10_vshmem_neighborhood_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape10_vshmem_neighborhood_lto.csv ./main \n",
    "cat rapport_etape10_vshmem_neighborhood_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0e44",
   "metadata": {},
   "source": [
    "## etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "**Description :** Combinaison : NVSHMEM avec recouvrement, synchrone de voisinage, et LTO pour maximiser la concurrence.\n",
    "\n",
    "**Intérêt :** Combinaison des meilleures pratiques pour un binaire ultra-optimisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash cd etape11_nvshmem_norm_overlap_neighborhood_sync_lto\n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv ./main\n",
    "cat rapport_etape11_nvshmem_norm_overlap_neighborhood_sync_lto.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33006f2a",
   "metadata": {},
   "source": [
    "## etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1\n",
    "**Description :** Tuning étendu : paramètres ajustables (taille de tuile, ordre de boucles) et hooks de benchmark.\n",
    "\n",
    "**Intérêt :** Ajout de paramètres de tuning et hooks de benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c846b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1 \n",
    "nv-nsight-cu-cli --csv --report-file rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv ./main\n",
    "cat rapport_etape12_nvshmem_norm_overlap_neighborhood_sync_lto_ext1.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
